[{"title":"Python爬虫","url":"/Python爬虫/","content":"\n- response对象的属性\n  - r = requests.get(url)\n  - r.headers()\n  - r.status_code = 200访问成功 = 404访问失败\n  - r.encoding 根据headers猜测响应内容编码方式\n    如果headers中没有charset，则r.encoding = iso-8859-1\n  - r.apparent_encoding根据内容猜测响应内容编码方式\n  - r.text 打印页面内容\n  - r.content 显示内容二进制形式（如图片资源）\n- requests处理异常\n  - requests.ConnectionError 网络连接错误异常，如DNS查询失败、拒接连接等\n  - requests.HTTPError HTTP错误异常\n    r.raise_for_status() 如果不是200，产生异常requests.HTTPError\n  - requests.URLRequired URL缺失异常\n  - requests.TooManyRedirects 超过最大重定向次数，产生重定向异常\n    对复杂连接进行访问\n  - requests.ConnectTimeout 连接远程服务器超时异常\n  - requests.Timeout 请求URL超时，产生超时异常\n- Requests库的7个主要方法\n  - requests.request() 构造一个请求，支撑以下各种方法的基础方法\n  - requests.get()\n  - requests.head()\n  - [requests.post](http://requests.post/)() 附加新的数据\n  - requests.put() 存储资源，覆盖原URL位置的资源\n  - requests.patch() 提交局部修改请求\n  - requests.delete()\n- URL格式 [http://host](http://host/)[:port][path] 缺省端口为80","tags":["学习笔记"]},{"title":"数据挖掘知识整理","url":"/数据挖掘知识整理/","content":"\n## 概述\n\n简单地说，数据挖掘就是从大量数据中自动发现或“挖掘”隐藏在数据背后的有用信息的过程\n$$\n数据集\\begin{cases}\n\t训练集 \\begin{cases}\n\t训练集：构建模型\n\t\\\\\n\t验证集：选择和调整模型\n\t\\end{cases}\n\\\\\\\\\n测试集：评估模型在新数据上的预测能力\n\\end{cases}\n$$\n\n| 数据集的一行 | 数据集的某列 |\n| :----------: | :----------: |\n|   数据对象   |     属性     |\n|     实例     |     变量     |\n|     记录     |      维      |\n|     观测     |     特征     |\n\n1. 按照数据集中是否已知目标变量的取值：\n   - 预测性挖掘任务(有监督学习)\n     根据某些属性的值预测特定属性的值，往往用于对新数据的预测.\n     被预测的属性一般称为目标变量(或因变量)，用来预测的变量称为输入变量(或自变量，说明变量)\n   - 描述性挖掘任务(无监督学习)\n     描述数据的特征\n     降维与可视化(数据预处理)\n     聚类分析\n     关联分析\n     孤立点检测\n2. 按照学习结果能否实时动态更新和应用，也是训练数据的方法\n   - 批量挖掘(线下学习)\n     先从历史数据集学习，再将学习结果应用于新数据.如果历史数据有更新，则需要重新学习.学习费事耗资源.\n   - 增量学习(在线学习)\n     从顺序输入的单个或小批量数据集不断学习，每次学习都廉价快速.适用于从不断变化的数据中学习，或历史数据集太大不能全部放入内存的情况.\n3. 按照学习方式的不同\n   - 基于模型的学习(理性主义)\n   - 基于实例的学习(经验主义)\n4. 按照目标变量的类型\n   - 回归(连续)\n     线性回归与多项式回归\n     分类与回归树<or>？？？</or>\n     SVR<or>？？？</or>\n     神经网络\n     K近邻  …\n   - 分类(离散)\n     逻辑回归\n     SVM\n     决策树与随机森林\n     神经网络\n     贝叶斯\n     K近邻 …\n\n## 数据探查与预处理\n\n数据探查的目的是：评估数据质量，发现数据问题、理解数据、为数据预处理提供依据和思路.\n\n- \n  数据建模之前使用数据预处理技术，减少噪声、不完整和不一致数据，可以提高模型的质量，进而提高数据挖掘的有效性和准确性。一个挖掘项目中需要的数据预处理与初始数据的质量、采用的分析方法、分析目标都有关系。\n\n- 数据清洗\n\n  前三项问题可以理解为数据存在噪声\n\n  - 处理错误或不一致的数据\n\n  - 处理缺失值\n\n    - 数据缺少的主要原因\n      - 数据不可得或获取成本太大\n      - 输入遗漏，理解错误，或传输错误等\n      - 属性值不存在（例如未婚者的配偶）\n    - 处理\n      - 不做处理\n      - 删除该条记录\n      - 填补\n        - 人工确定值或固定值\n        - 均值（中位数）或众数\n        - 同类别的均值（中位数）或众数\n        - 预测值：利用分类预测技术推断出最大可能取值\n      - 增加标识变量\n\n  - 识别处理孤立点\n\n    - 识别\n      - 常识\n      - 统计规则\n        盒图中：上下超过1.5倍箱体长度以外的点或：正太分布均值的3个标准差之外的点\n      - 聚类，…\n    - 处理\n      - 去除\n      - 视为噪声，进行平滑，盖帽法\n      - 保留\n\n  - 平滑噪声数据\n\n    噪声：变量取值存在的随机误差、不可理解的取值、错误、极端值\n\n    - 分箱\n\n      ![img](https://mubu.com/document_image/c7b185df-0e3d-4920-b5e2-b4ab856acd2e-2734354.jpg)\n\n      分箱方法利用数据的“近邻”（即周围的值）来平滑一组有序数据的值。首先对数据进行排序，并分配到具有相同高度（每箱数据项个数相同）或宽度（等距）的不同的“箱”中；其次通过箱子的平均值（Means）、中值（Median）、或者边界值等来进行平滑处理。一种观点：通过分箱个数（分箱后为有序变量）量化变量的权重\n\n      - 等高分箱：每箱数据个数相同\n      - 等宽分箱：每箱的箱距相同\n\n    - 聚类\n      聚类后找出孤立点，确定其存在的噪声属性，修改该属性值\n\n    - 回归\n\n- 数据集成\n\n  - 合并多个数据源中的数据，将之存放在一个一致的数据存储中\n    - 模式集成问题 ：同名不同义，同义不同名。\n    - 数据值冲突的检测与处理\n      例如：不同的计量单位、取值层次\n    - 数据冗余问题\n  - 纵向集成和横向集成\n\n- 数据消减\n\n  数据消减(data reduction)的目的就是缩小所挖掘数据的规模，但却不会影响(或基本不影响)最终的挖掘结果。 Aggregation（聚合）：例如：汇总数据代替明细数据，注意汇总的粒度；\n\n  - 维归约\n\n    - 属性构造\n\n      - 汇总属性的粒度\n        电话流失客户分月通话分钟数、电话流失客户分天通话分钟数\n\n    - 数据压缩（PCA、奇异值分解、小波变换等）\n\n      数据压缩是使用数据编码或变换，以便得到原数据的“压缩”表示。如果根据压缩的数据集可以恢复原来的数据集，则数据压缩是无损的，否则，数据压缩是有损的。\n\n      - 主成分分析\n        主成分分析是对于原先提出的所有变量，建立尽可能少的新变量，使得这些新变量是两两不相关的，而且这些新变量在反映课题的信息方面尽可能保持原有的信息\n      - 小波变换\n      - 奇异值分解\n\n    - 属性子集选择\n\n      - 手工消除无用或无关属性\n\n        - 具有唯一值或近似唯一值的变量\n        - 具有单一值或近似单一值的变量\n        - 可以相互转换或同意义的变量，存在函数依赖的变量\n\n      - 特征子集选取\n\n        特征子集选取就是选取最小的特征属性集合，得到的数据挖掘结果与所有特征参加的数据挖掘结果相近或完全一致。\n\n        - 和建模过程集成 （嵌入方法）\n          多元回归分析、决策树方法\n        - 进行独立的选取工作（过滤方法和包装方法）\n          例如：用关联分析选取重要变量、用决策树方法选取重要变量\n\n  - 行规约\n\n    - 抽样\n      - 简单随机抽样（有放回和无放回）\n      - 分层抽样\n      - 簇抽样\n      - 自适应或渐进抽样\n        Adaptive or progressive  sampling（自适应或渐进抽样）：it’sdifficulty to decide the number of observations in the sample, so at first  select little sample to mine, and thenenlarge the sample gradually and estimate the result model to get a appropriate（合适的）sample。\n    - 聚合\n    - 聚类\n\n- 数据转换\n\n  将购买量转换为  是否购买某类产品由身份证号推出身份或年龄或性别分布偏斜的取LOG，降低峰度取平方根\n\n  - 函数变换\n\n  - 数值数据规范化处理\n\n    不同变量常常具有不同的单位和不同的取值及变异程度。   例如：第1个变量的单位是kg，第2个变量的单位是cm，那么将第1个变量与第2个变量的值相加有何意义？  不同变量自身具有相差较大的变异时，会使在计算出的关系系数中，不同变量所占的比重大不相同。为了消除量纲影响和变量自身变异大小和数值大小的影响，需要将数据规范化。\n\n    - 最小-最大规范化\n\n      ![img](https://mubu.com/document_image/6782eb9b-5fa6-4139-ab48-b928a0a8474c-2734354.jpg)\n\n      A属性的原取值区间[minA，maxA] 目标新区间[new_minA, new_maxA]最小-最大规范化保留了原始数据值之间的关系。但是当有新数落在原数据区之外时，该方法将面临“越界”错误。另外，该方法受到孤立点的影响可能会比较大。\n\n    - z-score规范化（标准化）\n      将属性A的值v转换为标准化值v’​v’=(v-μ)/ σ\n\n    - 十进制缩放规范化\n\n      ![img](https://mubu.com/document_image/4393136a-9112-469a-896c-19166edfb284-2734354.jpg)\n\n      将每个数值除以10的相同次方取整，A的值v规范化为v'，举例如年龄分段\n\n  - 数值数据离散化\n\n    - 无监督离散化\n      - 二元化\n      - 分箱：等高、等宽、自定义\n    - 有监督离散化\n      - 基于熵\n        基于熵的离散化：先分成熵总和最小的两个区间，再将熵最大的区间分成两个小区间，依次分下去直到需要的区间数。思想同二叉决策树提高算法的效率\n\n  - 类别数据的数值化与数据泛化\n\n    - 名义变量（标称变量NOMINAL）的编码\n\n    - 有序变量（Odinal）的数值化\n\n      - 以顺序代替原值\n\n      - 规范化（将每个rif映射到[0,1]区间）\n\n        ![img](https://mubu.com/document_image/0dab63b2-a8c5-45fb-ab83-389715187b44-2734354.jpg)\n\n    - 类别数据的泛化\n      例如：地址：国家  省  地市   区县  街道\n\n- 复杂数据类型的预处理：特征提取\n\n### 数据质量\n\n- 数据的可用性\n  属性含义、类型、取值单位、范围及约束说明\n\n- 数据对应用的适合性\n\n- 相关性、完备性、时效性\n\n- 数据的代表性(抽样偏倚)\n\n- 数据收集问题\n  数据的重复、缺失、不一致问题\n\n### 属性特征\n\n- 属性的含义、取值单位（粒度）\n\n- 属性类型与测量水平\n\n  ![属性类型](\\img\\datamining\\var.png)\n\n- 是否存在缺失值\n\n- 描述性统计指标\n\n## 多元线性回归\n\n- 多元线性回归方程的形式和假定\n\n  ![img](https://mubu.com/document_image/f9dcfff5-9cb3-4a9d-9211-dedbfb445549-2734354.jpg)\n\n  其中ε代表其他所有别的因素的总和；\n\n  - 假定X和Y之间存在线性关系：散点图、相关系数、假设检验\n\n  - X相互独立：多重共线性检验（方差膨胀因子VIF）、变量选择法、回归优化法、pca等\n\n  -  ε独立同分布       ε ~ N（0，σ 2）\n\n  - 多项式回归\n\n    ![img](https://mubu.com/document_image/79239e7a-bd19-4c65-8658-45c2d2b04397-2734354.jpg)\n\n    在线性回归方程中加入输入变量的指数项（包括变量的交互项）；指数的最高值称为degree；\n\n- 方差分析与回归目标\n\n  ![img](https://mubu.com/document_image/f89398c4-be46-4606-8a39-09ff9bd10421-2734354.jpg)\n\n  - SST（total sum of squares）总离差和：反映了数据y1,y2,…,yn的波动大小；\n\n    ![img](https://mubu.com/document_image/546ddf3d-4a8a-4e6d-aa0c-190f7fbbc893-2734354.jpg)\n\n  - SSE（error sum of squares）残差平方和：表示X不能解释的误差部分，反映除去Y与X1,X2,…,Xp-1之间的线性关系以外的因素引起的数据y1,y2,…yn的波动；\n\n    ![img](https://mubu.com/document_image/5326185f-5e4a-49d7-a8c4-59b859fd89dd-2734354.jpg)\n\n  - SSR（regression sum of squares）回归平方和：反映了线性拟合值与它们的平均值的总偏差，即由变量X1,X2,…,Xp-1的变化引起的y1,y2…,yn的波动。若SSR=0，则每个拟合值相等，即y1,y2,…,yn不随着X1,X2,…,Xp-1的变化而变化\n\n    ![img](https://mubu.com/document_image/2bb19d69-4eaf-4f63-b688-80e90ef85585-2734354.jpg)\n\n  - 机器学习：MSE =SSE / n\n    MAE(平均绝对误差)\n\n  - 目标： min(MSE)（或min(SSE)）\n    成本函数（损失函数：cost（θ）=MSE （θ）\n\n- 回归参数求解\n\n  - 最小二乘\n\n    成本函数对参数求偏导，偏导为0，解方程组\n\n    - 回归方程的矩阵形式：Y=X θ \n\n      ![img](https://mubu.com/document_image/0e396e67-7bea-4564-9dbd-7ec0d11188a1-2734354.jpg)\n\n    - 自变量的相关性影响矩阵求逆\n\n    - 回归关系的显著性检验\n\n    - 回归系数的检验\n\n    - 误差项的正态性检验\n\n    - 最小二乘法矩阵求逆运算量大，所以适合于数据量小的情况\n\n  - 梯度下降\n\n    ![img](https://mubu.com/document_image/ad00819e-6f0e-4f72-a1e3-9f5b92298fd2-2734354.jpg)\n\n    不断更新θ，使损失函数趋向最小值\n\n    - 变动的方向：梯度负向\n\n    - 变动的大小：学习率η，学习率随着迭代减小\n\n    - 迭代次数：训练遍数epoch，设置一个较大的迭代次数，当梯度变化小于某个参数时停止迭代\n\n    - 输入变量不同取值范围的影响\n      各输入变量的取值范围差别较大时导致成本函数为椭圆盘，收敛需要更多的步数和时间。输入变量建模前需要规范化\n\n    - 批量梯度下降（Batch Gradient Desent）\n\n      ![img](https://mubu.com/document_image/11e8f99a-3921-4037-864b-d896ba5bf240-2734354.jpg)\n\n      θ的每次更新都需要所有训练集的实例进行计算；批量法是一定朝着正确的梯度的；\n\n    - 随机梯度下降（Stochastic Gradient Desent）\n      θ的每次更新只需要随机选择一个训练实例进行计算；成本函数收敛快，但不一定达到最小，当成本函数为不规则函数时有优势。一般学习率逐渐减小​；适合大规模数据集​\n\n    - 小批量梯度下降（ Mini-batch  Gradient Desent）\n      θ的每次更新选择一组训练实例进行计算；兼具批量梯度下降的稳定性和随机梯度下降的快速性​\n\n    - 超参：学习率；迭代次数；多项式迭代中指数的最高项degree；\n\n  - 其他方法\n\n    - 牛顿迭代法\n      利用目标函数f(x)的泰勒展开式并保留其线性部分来求解f(x)=0的近似解\n    - 牛顿法最值优化\n      求f’(x)=0时的根，使用了二阶导数，在每轮迭代中涉及海森矩阵的求逆，计算复杂度相当高，尤其在高维问题中几乎不可行。若能以较低的计算代价寻求海森矩阵的近似逆矩阵，则可以显著降低计算的时间，这就是拟牛顿法\n    - 常用的拟牛顿法有DFP、BFGS、SR1方法等\n\n- 模型评估与误差来源分析\n\n  - 训练误差：训练集上的模型预测误差\n\n  - 泛化误差：新数据集上的模型预测误差\n    测试集的误差作为泛化误差的无偏估计\n\n  - 评估\n\n    - 好的分类模型\n\n      ![img](https://mubu.com/document_image/6c85b495-859e-4423-91e0-cda43d5e2af8-2734354.jpg)\n\n      - 低训练误差\n      - 低泛化误差\n\n    - 拟合不足（欠拟合）：\n\n      ![img](https://mubu.com/document_image/d3591c77-00e9-40c0-800d-17a7c5d254cd-2734354.jpg)\n\n      - 较高训练误差\n      - 较高泛化误差\n\n    - 过拟合\n\n      ![img](https://mubu.com/document_image/29d78e7d-8b39-43c2-86ee-b03de63c31f3-2734354.jpg)\n\n      解决办法：简化模型\n\n      - 低训练误差\n      - 较高泛化误差\n\n  - 回归方程的评估复相关系数\n\n    ![img](https://mubu.com/document_image/dbf694c3-9466-488a-8753-1973f0aa0776-2734354.jpg)\n\n    在一个包含p-1个自变量的线性回归模型中（即模型中有p个参数），其中SSEp和SSRp分别表示拟合该模型的残差平方和及回归平方和，而总平方和SST是不随p变化的一个量；复相关系数越大则该回归方程描述因变量总变化量的比例越大；回归方程中不断添加自变量时，R2的值单调递增，因此，当所有M个自变量都在回归方程中，R2   的值最大；实际应用中，当p增加时，候选模型所对应的R2值一般开始增加较快，后逐渐趋于平缓，即增加自变量已不能显著提高拟合精度，则将由较快增加到趋于平缓的分界点处的R2值所对应的那个回归方程为最优的回归方程；\n\n  - 修正的复相关系数\n\n    ![img](https://mubu.com/document_image/37146158-7eb2-4385-962d-033aa7865878-2734354.jpg)\n\n    Rp中没有直接考虑模型中待估参数的个数P的作用。而一个好的模型应该既能充分反映 yi（1≤i≤n）的变化，又包含较少的待估系数（因而包括较少的自变量），因此将 P的控制引入到Rp中，得到它的一个修正量Ra\n\n  - 模型误差的来源\n\n    - bias\n      模型类型的偏差，bias越大模型拟合度越低，欠拟合的可能性越大\n    - Variance\n      模型复杂度造成的偏差，模型复杂度越大，variance越大，过拟合的可能性越大\n    - Irreducibleerror\n      数据本身造成的误差，是不可减少的误差\n\n- 回归方程选择与正则化\n\n  在一定的准则下选取对因变量影响较为显著的自变量，建立一个既合理又简单实用的回归模型\n\n  - 数据集\n\n    选出最佳模型参数后，根据整个训练集重新训练模型\n\n    - 训练集\n      - 训练集：练集构建模型\n      - 验证集：选择和调整模型\n    - 测试集：评估模型在新数据上的预测能力\n\n  - 逐步回归法\n\n    - 逐步回归法的基本步骤是依次拟合一系列回归方程，后一个回归方程在前一个的基础上增加或删除一个自变量\n\n    - 增加或删除某个自变量的准测用残差平方和的增加或减少量来衡量\n\n    - 一般采用偏F检验统计量\n\n      ![img](https://mubu.com/document_image/8ed53585-c9c9-453e-9f65-2af9405eaa2f-2734354.jpg)\n\n  - 正则化（预防过拟合）\n\n    在模型拟合度和模型复杂度之间进行平衡\n\n    - 岭回归\n\n      ![img](https://mubu.com/document_image/e7748df9-fc2e-4dde-8987-ba8679e73602-2734354.jpg)\n\n    - Lasso回归\n\n      ![img](https://mubu.com/document_image/e84b49e6-7a21-4861-ab60-bf7cb72ddda6-2734354.jpg)\n\n      Lasso变量筛选能力强\n\n    - Elastic Net\n\n      ![img](https://mubu.com/document_image/4c0006e8-d441-4860-8258-bc9a818132a3-2734354.jpg)\n\n- 其它问题\n\n  - 缺失值对回归的影响\n\n  - 类别变量如何加入回归方程\n\n    - 名义变量的编码\n      - one-hot编码\n      - 线性回归中，为避免k个二元变量的完全线性相关，往往取消1个二元变量（其对应取值作为参照水平）\n    - 序数变量的编码\n      pranks(职称)：助教、讲师、副教授、教授顺序编码：  0、1、2、3\n\n  - 回归系数如何理解？\n    系数:  影响方向、影响量自变量增减一个单位，因变量的变动量对类别变量，则表示当前水平相对于参照水平，因变量的变动量\n\n  - 数值属性的重要性\n\n    数值变量的规范化\n\n    - 标准化\n      v’=(v-μ)/ σ\n\n    - 最大最小规范化\n\n      ![img](https://mubu.com/document_image/d30f4fbd-aeb4-4644-a534-12c5d2eebd1f-2734354.jpg)\n\n## 逻辑回归\n\n## 决策树\n\n## 贝叶斯-k近邻-SVM\n\n## 分类器的评估与不平衡分布类\n\n### 决策阈值\n\n二分类的决策阈值：**t=0.5**\n\n### 分类器的评估指标\n\n**二分类问题的混淆矩阵**  \n\n|       | 预测+ | 预测- | 总    |\n| :---- | ----- | ----- | ----- |\n| 实际+ | tp    | fn    | tp+fn |\n| 实际- | fp    | tn    | fp+tn |\n| 总    | tp+fp | fn+tn | all   |\n\n准确率:  (tp+tn)/all\t\t(识别率)\n误分类率：(fp+fn)/all\n\nPrecision：返回的结果中真正和信息需求相关的文档所占的百分比tp/(tp+fp)\nrecall：所有和信息需求真正相关的文档中被检索系统返回的百分比tp/(tp+fn)\n\n真正率：tp/(tp+fn) \t\t\ttp/实际+\t\t\t（灵敏度）\n真负率：tn/(fp+tn)\t\t\t tn/实际-\t\t\t（特指度）\n假正率：fp/(fp+tn)\t\t \tfp/实际-\n假负率：fn/(tp+fn)\t\t\t fn/实际+\n\n### 评估方法（验证集的划分方法）\n\n### 辅助图形\n\n### 不平衡分布类处理技术\n\n","tags":["专业课"]},{"title":"资料收集","url":"/资料收集/","content":"\nshift+enter测试\n不会缩进的效果\n\nenter效果","tags":["生活"]},{"title":"基于MapReduce的Kindle评论数据分析","url":"/基于MapReduce的Kindle评论数据分析/","content":"\n# Hadoop\n\n## **1**    **项目简介**\n\n### **1.1**    **背景**\n\n随着Web技术和电子商务的发展，越来越多的人会在线上对自己买过的商品发表评论。这些评论信息虽然可能会带着一些发布者的主观情感，但是能在一定程度上反映出商品的好坏以及顾客对商品的喜爱程度。基于大数据分析技术，能够从大量评论数据中分析出商品的整体评价、评论者的评论偏好等。如果能够去除评论中各种因素带来的偏差，得到的结果将能够更加准确地反映事实。\n\n商家得到从大量评论数据中分析出的顾客反馈后，将有助于商家进行销售策略的决策，而其他准备购买该商品的顾客也可以根据评论的反馈来帮助自己判断该商品是否值得购买。\n\n### **1.2**    **项目实现目的**\n\n基于MapReduce算法，通过不断地修改分析评论数据的标准来得到更加真实的分析结果。先通过分析不同时间粒度、不同评论星级下的评论分布以及各级评分商品的评论情况来得到数据的基本情况。在此基础上修正评分模型，剔除“水军”、对评论进行情感分析，结合时效性和帮助性来得到更加真实准确的评论情况。\n\n### **1.3**    **整体思路**\n\n本项目旨在分析亚马逊kindle商店1996年到2014年的百万条评论数据，修正评分模型从而得到更为真实准确的评论情况。分析过程分为以下步骤：\n\n（1）  针对评论原始数据（一级数据）进行分析，得到不同时间粒度下的评论分布、各个阶段评论的词云等二级数据。\n（2）  求出不同标准下的各个商品的平均评级作为基准对比数据。\n（3）  在基准数据的基础上，识别出评论中的“水军”，根据帮助度和时效性确定每条评论评级的权重，对评论进行情感识别分析，去除不同人之间的偏好评分差异，修正评分模型，重新对一级数据进行分析，得到更加准确的结果。\n\n以上的步骤都是基于Hadoop MapReduce编程求解，得出分析结果后使用Python制作出图表。\n\n最后，本文列出了各个阶段的分析结果。\n\n## **2**    **数据及集群配置说明**\n\n### **2.1**    **数据来源**\n\n我们从kaggle网站下载了有关亚马逊的kindle商店产品的评论数据，评论时间从1996年5月到2014年7月，总共982619条，其中每个评论者至少发布了5条评论，每个产品都至少有5条评论。\n\n### **2.2**    **数据结构说明**\n\n**数据格式:** json\n\n**数据字段：**\n\nasin:  产品编号  \nhelpful:  评论是否有帮助  \noverall:  产品评级  \nreviewText:  评论内容  \nreviewTime:  评论时间  \nreviewerID:  评论者ID  \nreviewerName:  评论者名称  \nsummary:  评论内容的概括  \nunixReviewTime:  时间戳\n\n注：helpful：[m,n]表示有n个人对这条评论进行了评价，其中m个人认为这条评论有帮助。\n\n**数据实例：**\n\n{\"reviewerID\":\"A1F6404F1VG29J\",\"asin\":\"B000F83SZQ\", reviewerName\": \"Avidreader\", \"helpful\": [0, 0], \"reviewText\": \"I enjoy vintage ... for me.\", \"overall\": 5.0, \"summary\": \"Nice vintage story\", \"unixReviewTime\": 1399248000, \"reviewTime\": \"05 5, 2014\"}\n\n### **2.3**    **集群配置说明**\n\n**集群搭建情况：**\n\n| 机器名称 | IP地址        | 硬件配置                |\n| -------- | ------------- | ----------------------- |\n| Master   | 192.168.1.246 | Core i5 双核   RAM:12GB |\n| Slaver0  | 192.168.1.243 | Core i5 双核   RAM:8GB  |\n| Slaver1  | 192.168.1.245 | Core i5 双核 RAM:8GB    |\n| Slaver2  | 192.168.1.247 | Core i5 双核   RAM:8GB  |\n\n\n\n**软件：**\n\n| 名称                   | **版本** |\n| ---------------------- | -------- |\n| Hadoop                 | 2.9.0    |\n| Ubuntu                 | 16.04.5  |\n| VMware Workstation Pro | 12.1.0   |\n| Python                 | 3.6.4    |\n| Maven                  | 3.6.0    |\n\n## **3**    **数据概览**\n\n经统计，共有评论者68223人，商品61934种。同时，没有同一评论者为同一商品留评超过两次。\n\n### **3.1**    **不同时间粒度下的评论分布**\n\n首先，在该案例中，我们首先需要了解在不同时间下评论数目的分布，从而对用户的活跃情况有一个初步认识，因为评论数目一定程度上影响了Kindle书籍的销售数目，因此提高Kindle用户的活跃度，可以提高Kindle书籍的销售情况。\n\n具体处理流程如下：\n\n1.入初始数据，根据评论的UNIX时间戳进行统计，以天作为时间粒度（原始数据中UNIX时间戳缺乏精细到小时的数据），统计出不同日期的评论数目。用例如下：\n\n**·参考代码** **CommentCountByDate.java**[^1]\n\n**输入：**{\"reviewerID\": \"A1F6404F1VG29J\", \"asin\": \"B000F83SZQ\", \"reviewerName\": \"Avidreader\", \"helpful\": [0, 0], \"reviewText\": \"I enjoy vintage ... for me.\", \"overall\": 5.0, \"summary\": \"Nice vintage story\", \"unixReviewTime\": 1399248000, \"reviewTime\": \"05 5, 2014\"} \n\n**输出：**key: 2000-03-05(评论日期)   value: 1（评论次数）\n\n2.在获取以天作为时间粒度，2000~2014年不同日期的评论数目后，利用Python统计出不同年份、月份、工作日的日平均评论数目：\n\n将统计结果绘制成折线图如下:\n\n![img](/img/hadoop/output_3_0.png)\n\n从上图中可以看出，Kindle的评论数目自从2010年开始呈现井喷式增长，最终在2014年达到日平均数目为2000次。这可能是因为随着移动互联网的发展，更多的人选择在kindle上购买电子书；\n\n![img](/img/hadoop/output_4_0.png)\n\n从上图中可以看出，日平均评论数目在1月份至7月份在525次上下波动；在8-12月份与之前相比显著下降，但仍然呈现出回升趋势。这可能是因为人们在年初的时候会一次性购买今年要阅读的书籍；\n\n![img](/img/hadoop/output_5_0.png)\n\n从上图中可以看出，在周日、周一时，日平均评论数目与平时相比较高。这可能是因为在周日、周一等人们有较多空闲的时间来阅读书籍和对书籍进行评论。\n\n因此，Kindle销售部门可在8-12月份增加促销活动；在周日、周一等人们有较多空闲的时间，增加商品推送，从而，提高Kindle书籍销量。\n\n### **3.2**   **各阶段评分商品的评论关键词**\n\n在获取不同商品的评分后，为分析不同评论的内容对商品评分的影响 ，我们根据商品评分的不同划分为五个区间 [1,2), [2,3), [3,4), [4,5)，[5,5]根据TF-IDF方法提取出每个评论的关键词，并统计不同评分区间下关键词的分布，绘制成图云加以展示。\n\n具体处理流程如下：\n\n**·参考代码** **CommentTermFrequency.java**[^2]\n\n1.在对评论进行分词并去除stopword后，计算出其词频；\n$$\nTF = \\frac{在某一条评论中单词w出现的次数}{该评论中去除stopword后的所有单词的数目}\n$$\n**输入：**{\"reviewerID\": \"A1F6404F1VG29J\", \"asin\": \"B000F83SZQ\", \"reviewerName\": \"Avidreader\", \"helpful\": [0, 0], \"reviewText\": \"I enjoy vintage ... for me.\", \"overall\": 5.0, \"summary\": \"Nice vintage story\", \"unixReviewTime\": 1399248000, \"reviewTime\": \"05 5, 2014\"} \n\n**输出：**key: nice B000F83SZQ+A1F6404F1VG29J   value: 0.10（词频）\n\n2.在获取该案例中评论的词频后，我们计算出每条评论中的词频-逆文件频率；\n$$\nTF - IDF_w = TF_w\\times lg\\frac{评论总数}{包含单词w的评论数}\n$$\n**输入：**nice B000F83SZQ+A1F6404F1VG29J    0.10（上一步骤中的输出） \n\n**输出：**nice B000F83SZQ+A1F6404F1VG29J    0.15 \n\n3.在得到每个评论中每个单词的词频-逆文件频率后，我们选取最高者，作为该评论的关键词；\n\n**输入：**nice B000F83SZQ+A1F6404F1VG29J    0.15 （上一步骤中的输出）\n\n**输出：** B000F83SZQ+A1F6404F1VG29J    nice \n\n4.在获取到每条评论的关键词后，根据该评论所评论的商品进行汇总，以降低文件大小，提高下一步骤中使用Python进行数据分析的效率；\n\n**输入：** B000F83SZQ+A1F6404F1VG29J    nice（上一步骤中的输出） \n\n**输出：** B000F83SZQ nice      5 \n\n5.根据前面计算出的不同商品的最终评分，利用Python对商品评论关键词的数目进行统计，得出不同评分区间下高频关键词的不同，进而分析评论内容对商品评分的影响。\n\n我们根据所得出的不同评分区间下关键词的个数，利用Python绘制成图云，如下所示：\n\n![img](/img/hadoop/level.jpg)\n\n从左到右、从上到下分布为商品评分在[1,2), [2,3), [3,4), [4,5)，[5,5]评论关键词的词云分布图。\n\n从图中，我们可以看出：\n\n•   因为一星评价的商品比较少，所以其评论数目也较少，且通过TF-IDF识别出来的关键字中多为姓名，不具有代表性；\n\n•   在二星、三星评价的商品中，多为waste、horrible、deleted等负面词语，与我们的认知相一致；\n\n•   在四星、五星评价的商品中，多为recipes、diet等食谱类的描述词语，可能食谱类书籍评分均较高；同时还存在着部分cute、Christmas、children、kids等词语，可能儿童类书籍评分较高。\n\n### **3.3**   **各个商品的基准平均评级**\n\n读入原始数据，以商品编号作为键，商品评级作为值，利用MapReduce计算每种商品的平均评级：\n\n**·参考代码** **Overall.java**[^3]\n\n**输入：**{\"reviewerID\": \"A1F6404F1VG29J\", \"asin\": \"B000F83SZQ\", \"reviewerName\": \"Avidreader\", \"helpful\": [0, 0], \"reviewText\": \"I enjoy vintage ... for me.\", \"overall\": 5.0, \"summary\": \"Nice vintage story\", \"unixReviewTime\": 1399248000, \"reviewTime\": \"05 5, 2014\"} \n\n**输出：**key: B000F83SZQ（商品编号）  value: 4.89（商品评级）\n\n得到每种商品的平均评级，部分示例如下：\n\nB000QCS8YM 5.0 \nB000QFOD8E 3.5 \nB000QUCOB2 4.285714 \nB000R3NNAE 4.4444447 \nB000R93D4Y 3.96 \nB000R93D8U 3.5 \n\n之后利用商品及其评级，统计各阶段评级的商品总数，商品最终评级视为评级整数部的数值大小：\n\n**·参考代码** **CountOverall.java**[^4]\n\n**输入：**B000F83SZQ  4.89 \n\n**输出：**key: 1（商品等级）  value: 64（商品数量） \n\n利用Python统计出评级各阶段的商品总数目，并绘制成柱状图如下（Python代码：print_models.py）：\n\n![img](/img/hadoop/overallBase.png)\n\n| 评级     | 1    | 2    | 3     | 4     | 5    |\n| -------- | ---- | ---- | ----- | ----- | ---- |\n| 商品总数 | 64   | 823  | 10734 | 45596 | 4717 |\n\n从上图中可以看出，Kindle的各商品评级普遍较高，集中在4星左右，一星和二星的商品较少，存在一定数量的5星商品。\n\n## **4**    **商品评级修正**\n\n### **4.1**   **基于评论次数与整体评级的水军识别**\n\n1.利用之前写的统计不同日期评论数的代码，增加条件，筛选出评论数超过20条的记录。\n\n**·参考代码** **CommentCountByDateOver20.java**\n\n**输入：**{\"reviewerID\": \"A1F6404F1VG29J\", \"asin\": \"B000F83SZQ\", \"reviewerName\": \"Avidreader\", \"helpful\": [0, 0], \"reviewText\": \"I enjoy vintage ... for me.\", \"overall\": 5.0, \"summary\": \"Nice vintage story\", \"unixReviewTime\": 1399248000, \"reviewTime\": \"05 5, 2014\"} \n\n**输出：**key: 2000-03-05   value: 1 \n\n2.输入原始数据，计算每位评论者每天的评论总数，当一位评论者在一天之内评论总数超过一定数目α，或者评论总数占比当天评论总数超过某个阈值β，我们怀疑该评论者可能是职业刷评人员，最后用函数整理结果，得到这些人的评论ID。\n\n$$\n某人当天评论数 > \\alpha \\quad||\\quad\\frac{某人当天评论数}{当天评论总数} > \\beta\n$$\n**·参考代码** **IsPaidPoster1.java**\n\n**输入：**{\"reviewerID\": \"A1F6404F1VG29J\", \"asin\": \"B000F83SZQ\", \"reviewerName\": \"Avidreader\", \"helpful\": [0, 0], \"reviewText\": \"I enjoy vintage ... for me.\", \"overall\": 5.0, \"summary\": \"Nice vintage story\", \"unixReviewTime\": 1399248000, \"reviewTime\": \"05 5, 2014\"} \n\n**输出：**key: 2014-05-05,A1F6404F1VG29J   value: 20 \n\n设置α=15，β=0.25，得到潜在刷评人员60人，部分ID示例如下：\n\nA320TMDV6KCFU  A2YXNKWOGHP6AW   AAK9CEYIL2XL5   A9XRB71GIX26M AKE6711D72LTX  A1RXR105ND8OSH   A2AY83K9N60V38  A5IDWDZ2L1LA AAVZOTZWVQYRV  A1BSKHVCABJFXN 3FOL8CN5A1TFR\n\n3.输入原始数据，计算每位评论者所有评级的均值与方差，当一位评论者所有评级的均值大于4.9或小于1.1，并且所有评级的方差小于0.1，这些评论均视为恶评或好评，我们怀疑这些评论者可能是恶意刷评人员，即使不是，他们的评论也具有混淆产品评级的不当影响，我们将其等同于水军。最后用函数整理结果，只得到这些人的评论编号。为了加大该处理的可信度，我们只考虑了评论总数在100以上的评论者。\n\n$$\n\\begin{cases}\n某人所有评论评级的均值>4.9\\quad ||\\quad某人所有评论评级的均值<0.1 \\\\\n某人所有评论评级的方差 < 0.1 \\\\\n某人所有评论评级的数目 > 100\n\\end{cases}\n$$\n**·参考代码** **IsPaidPoster2.java**\n\n**输入：**{\"reviewerID\": \"A1F6404F1VG29J\", \"asin\": \"B000F83SZQ\", \"reviewerName\": \"Avidreader\", \"helpful\": [0, 0], \"reviewText\": \"I enjoy vintage ... for me.\", \"overall\": 5.0, \"summary\": \"Nice vintage story\", \"unixReviewTime\": 1399248000, \"reviewTime\": \"05 5, 2014\"} \n\n**输出：**key: A12E0W3IY6RJM   value: 4.9606986  0.037756715 \n\n最终得到潜在恶评人员144位，他们的部分ID如下：\n\nA107QCQSFVT6VN    A10C4X94VN9IG8    A1141DEY00149Z    A12E0W3IY6RJMA12NCJPL1Y3AH4    A13D5S4OXQUPRM    A141H51I3H4B1S A17YQQLSTXODGV A18L0QYPXCUD8E    A18YWXFF8FFGB2    A19BBQS3X97H2B\n\n将两种处理方式汇总，我们得到了199位潜在评论不当用户，其中5位兼具刷评与恶意评论两条性质，他们的ID以及昵称如下：\n\n| ID             | 昵称           |\n| -------------- | -------------- |\n| A1ENV91MFAEVA3 | JoT            |\n| A24F8TZ7WU4JDQ | Rebecca Palmer |\n| A28MPK002D2WJ1 | AA             |\n| A3M7QUA7XT7XIM | Jr.            |\n| A3PTWPKPXOG8Y5 | Alexis         |\n\n利用之前写的统计每种商品的平均评级的代码Overall.java，增加条件，剔除水军的评论，确保每条评论的有效性。得到新的每种商品的评论评级。\n\n**·参考代码** **OverallClean.java**\n\n**输入：**{\"reviewerID\": \"A1F6404F1VG29J\", \"asin\": \"B000F83SZQ\", \"reviewerName\": \"Avidreader\", \"helpful\": [0, 0], \"reviewText\": \"I enjoy vintage ... for me.\", \"overall\": 5.0, \"summary\": \"Nice vintage story\", \"unixReviewTime\": 1399248000, \"reviewTime\": \"05 5, 2014\"} \n\n**输出：**key: B000F83SZQ（商品编号）  value: 4.25（商品评级）\n\n利用商品及其评级，重新统计各阶段评级的商品总数，商品评级整数部的数值大小视为最终评级。\n\n| 评级     | 1    | 2    | 3     | 4     | 5    |\n| -------- | ---- | ---- | ----- | ----- | ---- |\n| 商品总数 | 70   | 872  | 11056 | 45033 | 4902 |\n\n我们发现经过处理，商品总数减少了1个，对应的事实便是，少了的商品的全部评论均由水军带来，或者说均是我们认定的无效评论。该商品的AsinID及平均评级为：\n\nB00IT2GHAS 5.0\n\n截止2018年12月，该商品具体信息如下：\n\n![img](/img/hadoop/gay.png)\n\n可以发现，现阶段该商品增加了许多1星评论，而且它的评级呈现5星和1星的两极化趋势，所以，该商品很可能符合我们的猜想，全部评论均由水军带来，存在不实评价的情况。\n\n### **4.2**   **基于评论时效性及帮助度的评分修正**\n\n依据评论的帮助评分与评论时间，赋予每个评级不同帮助度与时效性的权重，重新计算剔除水军后的商品评级。\n\n输入原始数据，利用之前写的统计剔除水军后的每种商品的平均评级的代码OverallClean.java，融入每条评论评级权重信息，重新计算每种商品评级。参考参数helpful及unixReviewTime，计算每条评论的帮助度以及时效性。\n\n【帮助度】当一条评论没有帮助度时，我们将本条关于该商品的评论设为所有其他有帮助度的均值，平均其重要度。取值区间为(0, 1]；\n\n【时效性】我们用评论时间戳比上最新的2014-7-23，即unixTime=1406073600的结果再平方（以增加区别度），当做时效性的参数。取值区间为(0, 1]；\n\n$$\n\\begin{cases}\n针对同一商品，每条评论评级的得分为ax_i + by_i \\\\\n针对同一商品，每条评论评级的权重为\\frac{ax_i + by_i}{(ax_1 + by_1 + \\cdots + ax_n + by_n)}\n\\end{cases}\\\\\n其中，帮助度为x,时效性为y\n$$\n设置帮助度系数a=1，时效性系数为b=1，即，我们认为帮助度与时效性同等重要。\n\n**·参考代码** **OverallWeights.java**\n\n**输入：**{\"reviewerID\": \"A1F6404F1VG29J\", \"asin\": \"B000F83SZQ\", \"reviewerName\": \"Avidreader\", \"helpful\": [0, 0], \"reviewText\": \"I enjoy vintage ... for me.\", \"overall\": 5.0, \"summary\": \"Nice vintage story\", \"unixReviewTime\": 1399248000, \"reviewTime\": \"05 5, 2014\"} \n\n**输出：**key: B000F83SZQ（商品编号）  value: 4.249840640960779（商品评级）\n\n利用商品及其评级，重新统计各阶段评级的商品总数，商品评级整数部的数值大小视为最终评级。\n\n| 评级     | 1    | 2    | 3     | 4     | 5    |\n| -------- | ---- | ---- | ----- | ----- | ---- |\n| 商品总数 | 88   | 1086 | 12775 | 43082 | 4902 |\n\n利用Python绘制成折线图，对比三阶段的商品评级统计结果。\n\n首先是各阶段评级商品总数（Python代码：print_zhe.py）：\n\n![img](/img/hadoop/overall.png)\n\n| 评级           | 1    | 2    | 3     | 4     | 5    |\n| -------------- | ---- | ---- | ----- | ----- | ---- |\n| overall        | 64   | 823  | 10724 | 45596 | 4717 |\n| overallClean   | 70   | 872  | 11056 | 45033 | 4902 |\n| overallWeights | 88   | 1086 | 12775 | 43082 | 4902 |\n\n经过不断的调整，四星级商品数在逐渐下滑到三星级，一星级、二星级商品数也有微量增多，我们的评级系统变得越来越严格。\n\n同时，我们观察到评级为5的商品数目在剔除水军后增加了一部分，我们分析可能是有竞争对手恶意刷评导致评分下降。然而进行了权重修改后评级为5的商品数目没有改动，这是因为评级为5的商品必须符合每条评论均为5星，这样，无论我们如何修改权重，都无法改变商品的评级。\n\n另外，我们认为评论数与销售量在一定程度上成正比，所以我们将平均评论量类比为商品受欢迎程度，以下是各阶段商品的平均评论条数（保留两位小数）。\n\n**·参考代码** **AvgReview.java**\n\n![img](/img/hadoop/overallAvgReview.png)\n\n| 评级           | 1    | 2    | 3     | 4     | 5    |\n| -------------- | ---- | ---- | ----- | ----- | ---- |\n| overall        | 7.06 | 9.54 | 14.64 | 16.54 | 6.62 |\n| overallClean   | 7.07 | 9.53 | 14.54 | 16.63 | 6.58 |\n| overallWeights | 6.91 | 8.91 | 13.83 | 16.98 | 6.58 |\n\n整体上分析我们发现，1~4级评级递增时，平均评论量也是依次递增的，符合了我们的假设，即评论数与销售量在一定程度上成正比。反观5星评级的商品却又有最低的评论量，我们猜测5星评级商品会更少有争议，少了大量的吐槽评论，购买者意见较为一致，会更多的选择点赞帮助评分，而不是自行评价，同时会有更多的回购用户，而回购用户往往不会进行二次评价。\n\n​    分开来看三条折线，经过不断的调整，1~4星的平均评论量递增曲线变得愈加陡峭，具体表现为评级为1~3的商品平均评论量均在不断下降，而评级为4的商品平均评论量在不断上升。评级不同，商品平均评论量的差距也在拉大。\n\n### **4.3**   **基于**CoreNLP的评论情感识别\n\n正如我们前面的发现，评级为5的商品是无论如何都不会再被降级的，那么对于那些商品普通，评论很少，评论里描述平平的，就因为仅有的几条评论就能保持5星的情况，显然是不符合实际的。这些评级本身就十分不严谨，外加已经存在评级为5星的商品却拥有最低的商品平均评论量的现状，我们意识到了每条评论的星级本身也需要具有准确性。\n\n我们还发现，每个人的评论标准都不大相同，有的人要求严格，评分整体偏低，有的人比较随意，评分忽高忽低，所以，我们有必要给出科学的评级方式，再结合原本的评级，对每条评论本身的星级进行修改。\n\n情感分析是指利用机器提取人们对某人或事物的态度，从而发现潜在的问题用于改进或预测。商品的评价分为星级评价和文本评价。星级评价和好评率在排序算法中占据重要地位。有的商家为了提升排名，采取“五星好评返现”的方式来诱导顾客好评，但实际上用户可能对商品并不满意，因此会在文本评论或者追加评论时说出对商品不满意的真实评价。因此，对评论文本进行情感分析并使用文本评论好评率来对商品进行重新排序，指导人们根据真实评价选取商品就很显得有意义。本文中，利用CoreNLP包对商品评论进行情感分析，以对商品评分进行矫正。\n\n为简化模型，我们做出如下假设：\n\n• 评论中长度最长的一句话代表该评论的情感；\n\n由于，该模型较为复杂，需要运行较长的时间，我们仅运行前1000条的评论数据。\n\n**·参考代码****SentimentAnalysis.java**\n\n**输入：**{\"reviewerID\": \"A1F6404F1VG29J\", \"asin\": \"B000F83SZQ\", \"reviewerName\": \"Avidreader\", \"helpful\": [0, 0], \"reviewText\": \"I enjoy vintage ... for me.\", \"overall\": 5.0, \"summary\": \"Nice vintage story\", \"unixReviewTime\": 1399248000, \"reviewTime\": \"05 5, 2014\"} \n\n**输出：**key: B000F83SZQ+A1F6404F1VG29J   value: 2 \n\n输出的value值表示评论的正负面情绪，其中，1~5分别代表极度负面、负面、中性、正面、极度正面）\n\n列出矫正示例如下：\n\n| 商品编号   | 评论者ID      | 原评级 | 矫正评级 |\n| :--------- | ------------- | :----- | :------- |\n| B000FA64PA | AQZH7YTWQPOBE | 4      | 2        |\n\n内容：This is a short story focused on Darth Maul's role in helping the Trade Federation gain a mining colony. It's not bad, but it's also nothing exceptional. It's fairly short so we don't really get to see any characters develop. The few events that do happen seem to go by quickly, including what should have been major battles. The story is included in the novelShadow Hunter (Star Wars: Darth Maul), which is worth reading, so don't bother to buy this one separately.\n\n| 商品编号   | 评论者ID      | 原评级 | 矫正评级 |\n| ---------- | ------------- | ------ | -------- |\n| B000FA64PK | A2QK1U70OJ74P | 5      | 5        |\n\n内容：Excellent! Very well written story, very exciting with LOTS of action. The bad guys trying to kill Leia and Han. Viki Shesh is introduced.\n\n### **4.4**   **基于时效性、帮助性、情感识别的书籍评分修正**\n\n依据情感分析得到的矫正评级，以及原评级，综合之前的纠正方法，重新计算情绪分析1000条数据的商品评级。\n\n**·参考代码** **OverallSentiment.java**\n\n得到新的对比数据：各评级商品数目、各评级平均评论量，结果如下：\n\n| 1000条数据处理结果 | 1    | 2    | 3     | 4     | 5    |\n| ------------------ | ---- | ---- | ----- | ----- | ---- |\n| 原（商品数目）     | -    | 1    | 37    | 50    | 5    |\n| 矫正（商品数目）   | -    | 10   | 75    | 8     | -    |\n| 原（平均评论量）   | -    | 15.0 | 9.51  | 11.66 | 5.6  |\n| 矫正（平均评论量） | -    | 10.4 | 10.96 | 6.5   | -    |\n\n由于数据量较少，整体分析我们发现针对原本的处理结果，商品数目是比较吻合之前的所有数据处理结果的趋势的，即商品数目集中处于4等评级左右，但是平均评论量就与之前的趋势不符，我们认为这可能是数据量少，导致数据采集不够随机而出现的偏差，所以平均评论量的测量并不能反映样本代表总体的结果，商品数目便成为了我们分析的重点。\n\n首先，我们看到情绪分析的结果对评级结果有十分大的改动。情绪分析后评级总体呈下降趋势，许多4星商品变成了3星商品，5星商品甚至完全消失，而2级商品仅有增加却没有下降，最终商品数目的数据集中点变成了3等评级。我们认为情绪分析的结果评级为5或1是鲜少的，即评论内容想要达到极度正面或极度负面是十分困难的，所以情绪分析的结果符合我们对偏好差异的修正，结果贴合现实中极少出现一致好评或一致恶评的判断情形，呈现比较中等客观的评分情形。\n\n### **4.5**   **部分得分较高电子书列表**\n\nB000QCS8YM\n\n![img](/img/hadoop/1.png)\n\nB000UH5Z3A\n\n![img](/img/hadoop/2.png)\n\nB002F0826C\n\n![img](/img/hadoop/3.png)\n\nB002RHP5TK\n\n![img](/img/hadoop/4.png)\n\n更多更好的书单请关注我们的输出结果中的OverallWeights\n\n## **5**    **总结和展望**\n\n### **5.1**    **问题及总结**\n\n问题：缺乏相对准确的客观评分，无法通过最小二乘法等估计出相对准确的各参数取值；\n\n问题：情绪分析的模型较为复杂，导致运算速度较慢，无法对全部数据进行分析，且情绪分析结果存在误差；\n\n问题：数据与当下的时间脱轨，未能完全反应当下的需求。\n\n总结：我们的项目实现了商品评论的修正，得到了相对真实的评论情况。由于缺乏相对准确的评分、及时的数据、简单有效的模型，所以导致我们的评分修正模型仍存在一定的误差。\n\n### **5.2**   **展望**\n\n· 通过人工对Kindle产品进行判断，得出相对准确的客观评分，从而通过最小二乘估计等办法，得出相对准确的参数估计值；\n\n· 优化情感分析算法，使之可以在更多的时间内能够处理更多的数据，且判断更加准确；\n\n· 搜集最近的数据，用以验证我们的评分修正模型的准确性；\n\n· 考虑用户之间的联系，通过网络自回归模型，对评分模型进行进一步的修正等。\n\n##  附录\n\n[^1]: CommentCountByDate.java\n\n```java\n/**\n * @author 龚\n * 根据时间戳计算每天的评论数目.\n */\npublic class CommentCountByDate {\n\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n        String[] otherArgs = (new GenericOptionsParser(conf, args)).getRemainingArgs();\n        if (otherArgs.length < 2) {\n            System.err.println(\"Usage: Comments count by year <in> [<in>...] <out>\");\n            System.exit(2);\n        }\n        Job job = Job.getInstance(conf, \"CommentCountByDate\");\n        job.setJarByClass(CommentCountByDate.class);\n        job.setMapperClass(TokenizerMapper.class);\n        job.setCombinerClass(CountReducer.class);\n        job.setReducerClass(CountReducer.class);\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n        for (int i = 0; i < otherArgs.length - 1; ++i) {\n            FileInputFormat.addInputPath(job, new Path(otherArgs[i]));\n        }\n        FileOutputFormat.setOutputPath(job, new Path(otherArgs[otherArgs.length - 1]));\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n\n    public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {\n        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n            String line = value.toString();\n            Gson gson = new Gson();\n            KindleReview kindleReview = gson.fromJson(line, KindleReview.class);\n            Long unixReviewTime = kindleReview.getUnixReviewTime();\n            String reviewTime = new SimpleDateFormat(\"yyyy-MM-dd\").format(new Date(unixReviewTime * 1000));\n            context.write(new Text(reviewTime), new IntWritable(1));\n        }\n    }\n\n    public static class CountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n        public void reduce(Text key, Iterable<IntWritable> values,\n                           Context context\n        ) throws IOException, InterruptedException {\n            int sum = 0;\n            for (IntWritable value : values) {\n                sum += value.get();\n            }\n            context.write(key, new IntWritable(sum));\n        }\n    }\n}\n```\n\nCommentByDate.py\n\n```python\n\"\"\"\n * @author 龚\n * 绘制数据概览的三张图.\n\"\"\"\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns;sns.set(style=\"whitegrid\")\n\ndata = pd.read_table('./data/count_comment_by_date', header=None)\ndata.columns = ['date', '']\ndata['date'] = pd.to_datetime(data['date'])\ndata = data.set_index('date')\n\nby_year = data.groupby(data.index.year).mean()\nplt.plot(by_year,\"-x\")\nplt.xlabel('Year')\nplt.ylabel('Comment Numbers Per Day')\nplt.show()\n\nby_month = data.groupby(data.index.month).mean()\nplt.plot(by_month,\"-x\")\nplt.xlabel('Month')\nplt.ylabel('Comment Numbers Per Day')\nplt.show()\n\nby_weekday = data.groupby(data.index.weekday).mean()\nby_weekday.index = ['Mon', 'Tues', 'Wed', 'Thur', 'Fri', 'Sat', 'Sun']\nplt.plot(by_weekday,\"-x\")\nplt.xlabel('Weekday')\nplt.ylabel('Comment Numbers Per Day')\nplt.show()\n```\n\n[^2]: CommentTermFrequency.java\n\n```java\n/**\n * @author 龚\n * 词频分析.\n */\npublic class CommentTermFrequency {\n    public static String[] stopwords = {\"a\", \"able\", \"about\", \"across\", \"after\", \"all\", \"almost\", \"also\", \"am\", \"among\",\n            \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"but\", \"by\", \"can\", \"cannot\", \"could\", \"dear\",\n            \"did\", \"do\", \"does\", \"either\", \"else\", \"ever\", \"every\", \"for\", \"from\", \"get\", \"got\", \"had\", \"has\", \"have\",\n            \"he\", \"her\", \"hers\", \"him\", \"his\", \"how\", \"however\", \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"just\",\n            \"least\", \"let\", \"like\", \"likely\", \"may\", \"me\", \"might\", \"most\", \"must\", \"my\", \"neither\", \"no\", \"nor\", \"not\",\n            \"of\", \"off\", \"often\", \"on\", \"only\", \"or\", \"other\", \"our\", \"own\", \"rather\", \"said\", \"say\", \"says\", \"she\",\n            \"should\", \"since\", \"so\", \"some\", \"than\", \"that\", \"the\", \"their\", \"them\", \"then\", \"there\", \"these\", \"they\",\n            \"this\", \"tis\", \"to\", \"too\", \"twas\", \"us\", \"wants\", \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\",\n            \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"would\", \"yet\", \"you\", \"your\", \",\", \".\", \"!\", \"?\", \";\", \":\",\n            \"<\", \">\", \"/\", \"'\", \"\\\"\", \"\\\\\", \"[\", \"]\", \"{\", \"}\", \"|\", \"@\", \"#\", \"$\", \"%\", \"^\", \"&\", \"*\", \"(\", \")\", \"-\",\n            \"_\", \"=\", \"+\", \"~\", \"`\"};\n    public static Set<String> stopWordSet = new HashSet<String>(Arrays.asList(stopwords));\n\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n        String[] otherArgs = (new GenericOptionsParser(conf, args)).getRemainingArgs();\n        if (otherArgs.length < 2) {\n            System.err.println(\"Usage: CommentTermFrequency <in> [<in>...] <out>\");\n            System.exit(2);\n        }\n        Job job = Job.getInstance(conf, \"CommentTermFrequency\");\n        job.setJarByClass(CommentTermFrequency.class);\n        job.setMapperClass(TFMapper.class);\n        job.setCombinerClass(TFCombiner.class);\n        job.setReducerClass(TFReducer.class);\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(FloatWritable.class);\n        for (int i = 0; i < otherArgs.length - 1; ++i) {\n            FileInputFormat.addInputPath(job, new Path(otherArgs[i]));\n        }\n        FileOutputFormat.setOutputPath(job, new Path(otherArgs[otherArgs.length - 1]));\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n\n    public static class TFMapper extends Mapper<Object, Text, Text, FloatWritable> {\n        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n            String line = value.toString();\n            Gson gson = new Gson();\n            KindleReview kindleReview = gson.fromJson(line, KindleReview.class);\n            String ReviewText = kindleReview.getReviewText();\n            String location = kindleReview.getAsin() + \"+\" + kindleReview.getReviewerID();\n            Properties props = new Properties();\n            props.put(\"annotators\", \"tokenize, ssplit, pos, parse, lemma\");\n            StanfordCoreNLP pipeline = new StanfordCoreNLP(props);\n            Annotation annotation = pipeline.process(ReviewText);\n            List<CoreMap> sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);\n            int wordCount = 0;\n            for (CoreMap sentence : sentences) {\n                for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {\n                    String lemma = token.get(CoreAnnotations.LemmaAnnotation.class).toLowerCase();\n                    if (!stopWordSet.contains(lemma)) {\n                        context.write(new Text(location + \" \" + lemma), new FloatWritable(1));\n                        wordCount++;\n                    }\n                }\n            }\n            // 主要这里值使用的 \"!\"是特别构造的。 因为!的ascii比所有的字母都小。\n            context.write(new Text(location + \" !\"), new FloatWritable(wordCount));\n        }\n    }\n\n    public static class TFCombiner extends Reducer<Text, FloatWritable, Text, FloatWritable> {\n\n        float wordCount = 0;\n\n        public void reduce(Text key, Iterable<FloatWritable> values, Context context) throws IOException, InterruptedException {\n            int index = key.toString().indexOf(\" \");\n            // 因为!的ascii最小，所以在map阶段的排序后，!会出现在第一个\n            if (key.toString().substring(index + 1, index + 2).equals(\"!\")) {\n                for (FloatWritable value : values) {\n                    wordCount = value.get();\n                }\n                return;\n            }\n            float sum = 0;\n            for (FloatWritable value : values) {\n                sum += value.get();\n            }\n            float tf = sum / wordCount;\n            context.write(key, new FloatWritable(tf));\n        }\n    }\n\n    public static class TFReducer extends Reducer<Text, FloatWritable, Text, FloatWritable> {\n        public void reduce(Text key, Iterable<FloatWritable> values, Context context) throws IOException, InterruptedException {\n            for (FloatWritable value : values) {\n                context.write(key, value);\n            }\n        }\n    }\n}\n```\n\nCommentTFIDF.java\n\n```java\n/**\n * @author 龚\n * 在获取该案例中评论的词频后，我们计算出每条评论中的词频-逆文件频率${TF\n * IDF}_w=TF_w\\times\\lg\\frac{评论总数}{包含单词w的评论数}$； \n * 输入：nice B000F83SZQ+A1F6404F1VG29J    0.10（上一步骤中的输出）\n * 输出：nice B000F83SZQ+A1F6404F1VG29J    0.15\n */\npublic class CommentTFIDF { \n    public static void main(String[] args) throws Exception { \n        Configuration conf = new Configuration(); \n        String[] otherArgs = (new GenericOptionsParser(conf, args)).getRemainingArgs(); \n        if (otherArgs.length < 2) { \n            System.err.println(\"Usage: CommentTFIDF <in> [<in>...] <out>\"); \n            System.exit(2); \n        } \n        Job job = Job.getInstance(conf, \"CommentTFIDF\"); \n        job.setJarByClass(CommentTFIDF.class); \n        job.setMapperClass(TFIDFMapper.class); \n        job.setReducerClass(TFIDFReducer.class); \n        job.setOutputKeyClass(Text.class); \n        job.setOutputValueClass(Text.class); \n        for (int i = 0; i < otherArgs.length - 1; ++i) { \n            FileInputFormat.addInputPath(job, new Path(otherArgs[i])); \n        } \n        FileOutputFormat.setOutputPath(job, new Path(otherArgs[otherArgs.length - 1])); \n        System.exit(job.waitForCompletion(true) ? 0 : 1); \n    } \n    public static class TFIDFMapper extends Mapper<Object, Text, Text, Text> { \n        public void map(Object key, Text value, Context context) throws IOException, InterruptedException { \n            String line = value.toString().replaceAll(\"\\t\", \" \"); \n            String[] strs = line.split(\" \"); \n            // 获取单词 作为key \n            String word = strs[1]; \n            // 其余部分 作为value，统计单词在所有评论中出现的次数, “1” 表示出现一次。 \n            String other = strs[0] + \" \" + strs[2] + \" 1\"; \n            context.write(new Text(word), new Text(other)); \n        } \n    } \n    public static class TFIDFReducer extends Reducer<Text, Text, Text, Text> { \n        //总评论数 \n        int file_count = 982619; \n \n        public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException { \n            float sum = 0; \n            List<String> vals = new ArrayList<>(); \n            for (Text value : values) { \n                String[] strs = value.toString().split(\" \"); \n                // 统计此单词在所有评论中出现的次数 \n                try { \n                    sum += Integer.parseInt(strs[2]); \n                    vals.add(strs[0] + \" \" + strs[1]); \n                } catch (Exception e) { \n                    System.out.println(e); \n                } \n            } \n            // 单词在所有评论中出现的次数除以总评论数 = IDF \n            double idf = Math.log10(file_count / sum); \n            String word = key.toString(); \n            for (String val : vals) { \n                String[] strs = val.split(\" \"); \n                float tf = Float.parseFloat(strs[1]); \n                String loc = strs[0]; \n                String tfidf = \"\" + (tf * idf); \n                context.write(new Text(loc + \" \" + word), new Text(tfidf)); \n            } \n        } \n    } \n}\n```\n\nCommentAbstract.java\n\n```java\n/**\n * @author 龚\n * 在得到每个评论中每个单词的词频-逆文件频率后，我们选取最高者，作为该评论的\n * 键词.\n * 输入：nice B000F83SZQ+A1F6404F1VG29J    0.15（上一步骤中的输出） \n * 输出：B000F83SZQ+A1F6404F1VG29J    nice \n */\npublic class CommentAbstract { \n    public static void main(String[] args) throws Exception { \n        Configuration conf = new Configuration(); \n        String[] otherArgs = (new GenericOptionsParser(conf, args)).getRemainingArgs(); \n        if (otherArgs.length < 2) { \n            System.err.println(\"Usage: CommentAbstract <in> [<in>...] <out>\"); \n            System.exit(2); \n        } \n        Job job = Job.getInstance(conf, \"CommentAbstract\"); \n        job.setJarByClass(CommentAbstract.class); \n        job.setMapperClass(AbstractMapper.class); \n        job.setReducerClass(AbstractReducer.class); \n        job.setOutputKeyClass(Text.class); \n        job.setOutputValueClass(Text.class); \n        for (int i = 0; i < otherArgs.length - 1; ++i) { \n            FileInputFormat.addInputPath(job, new Path(otherArgs[i])); \n        } \n        FileOutputFormat.setOutputPath(job, new Path(otherArgs[otherArgs.length - 1])); \n        System.exit(job.waitForCompletion(true) ? 0 : 1); \n    } \n \n    public static class AbstractMapper extends Mapper<Object, Text, Text, Text> { \n        public void map(Object key, Text value, Context context) throws IOException, InterruptedException { \n            String line = value.toString().replaceAll(\"\\t\", \" \"); \n            String[] strs = line.split(\" \"); \n            // 获取评论标记作为key \n            String loc = strs[0]; \n            // 其余部分 作为value \n            String other = strs[1] + \" \" + strs[2]; \n            context.write(new Text(loc), new Text(other)); \n        } \n    } \n \n    public static class AbstractReducer extends Reducer<Text, Text, Text, Text> { \n        public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException { \n            float max = 0; \n            String abst = \"\"; \n            for (Text value : values) { \n                String[] strs = value.toString().split(\" \"); \n                String word = strs[0]; \n                float tfidf = Float.parseFloat(strs[1]); \n                if (tfidf > max) { \n                    max = tfidf; \n                    abst = word; \n                } \n            } \n            context.write(key, new Text(abst)); \n        } \n    } \n}\n```\n\nAsinAbstractCount.java\n\n```java\n/**\n * @author 龚\n * 在获取到每条评论的关键词后，根据该评论所评论的商品进行汇总，以降低文件大\n * 小，提高下一步骤中使用`Python`进行数据分析的效率.\n * 输入：B000F83SZQ+A1F6404F1VG29J    nice（上一步骤中的输出）\n * 输出：B000F83SZQ nice 5\n */\npublic class AsinAbstractCount { \n    public static void main(String[] args) throws Exception { \n        Configuration conf = new Configuration(); \n        String[] otherArgs = (new GenericOptionsParser(conf, args)).getRemainingArgs(); \n        if (otherArgs.length < 2) { \n            System.err.println(\"Usage: AsinAbstractCount <in> [<in>...] <out>\"); \n            System.exit(2); \n        } \n        Job job = Job.getInstance(conf, \"AsinAbstractCount\"); \n        job.setJarByClass(AsinAbstractCount.class); \n        job.setMapperClass(CountMapper.class); \n        job.setReducerClass(CountReducer.class); \n        job.setOutputKeyClass(Text.class); \n        job.setOutputValueClass(IntWritable.class); \n        for (int i = 0; i < otherArgs.length - 1; ++i) { \n            FileInputFormat.addInputPath(job, new Path(otherArgs[i])); \n        } \n        FileOutputFormat.setOutputPath(job, new Path(otherArgs[otherArgs.length - 1])); \n        System.exit(job.waitForCompletion(true) ? 0 : 1); \n    } \n \n    public static class CountMapper extends Mapper<Object, Text, Text, IntWritable> { \n        public void map(Object key, Text value, Context context) throws IOException, InterruptedException { \n            String line = value.toString().replaceAll(\"\\t\", \" \"); \n            String[] strs = line.split(\" \"); \n            // 获取asin，word记作为key \n            String asin = strs[0].substring(0, strs[0].indexOf('+')); \n            String word = strs[1]; \n            context.write(new Text(asin + \" \" + word), new IntWritable(1)); \n        } \n    } \n \n    public static class CountReducer extends Reducer<Text, IntWritable, Text, IntWritable> { \n        public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException { \n            int sum = 0; \n            for (IntWritable value : values) { \n                sum += value.get(); \n            } \n            context.write(key, new IntWritable(sum)); \n        } \n    } \n}\n```\n\n[^3]: Overall.java\n\n```java\n/**\n * @author 杨\n * 计算商品平均评级.\n */\npublic class Overall{\n\n    public static class MyMap extends Mapper<Object, Text, Text, IntWritable> {\n    \n        public void map(Object key, Text value, Context context) \n                throws IOException, InterruptedException {\n            \n            String review = value.toString();\n            Gson gson = new Gson();\n            KindleReview kindleReview = gson.fromJson(review, KindleReview.class);\n            int overall = kindleReview.getOverall();\n            String asin = kindleReview.getAsin();\t\t\n\t        context.write(new Text(asin), new IntWritable(overall));            \n        }\n    }\n\n  \n    public static class MyReduce extends\n            Reducer<Text,IntWritable, Text, Text> {\n\n        public void reduce(Text key, Iterable<IntWritable> values,\n                Context context) throws IOException, InterruptedException {\n            int sum = 0;\n\t        int i=0;\n\t        float ave=0;\n            for (IntWritable val : values) {\n                sum += val.get();\n\t\t        i++;\n            }\n\t        float s=(float)sum;\n\t        float j=(float)i;\n\t        ave=s/j;\n            context.write(key, new Text(\"\" + ave));\n        }\n    }\n\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n        Job job = new Job(conf,\"overall\"); \n        job.setJarByClass(Overall.class);\t\n        job.setMapperClass(MyMap.class);\n        job.setReducerClass(MyReduce.class);\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(Text.class);\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(IntWritable.class);\n        FileInputFormat.addInputPath(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n\n    }\n\n}\n```\n\n[^4]: CountOverall.java\n\n```java\n/**\n * @author 杨\n * 计算每个商品评级的商品总数.\n */\npublic class CountOverall{\n   \n    public static class MyMap extends Mapper<Object, Text, Text, IntWritable> {\n        \n        private final static IntWritable one = new IntWritable(1); \n        private Text word = new Text(); \n\n    \n        public void map(Object key, Text value, Context context) \n                throws IOException, InterruptedException {\n            \n\t\t    String[] str = value.toString().split(\"\\\\s+\");\n\t\t    double i= Double.parseDouble(str[1]);\n\t\t    int j=(int)i;\n            word.set(\"\" + j);\t    \t\n\t    \tcontext.write(word,one);\n       \n        }\n    }\n\n   \n    public static class MyReduce extends\n            Reducer<Text, IntWritable, Text, IntWritable> {\n\n        public void reduce(Text key, Iterable<IntWritable> values,\n                Context context) throws IOException, InterruptedException {\n            int sum = 0;\n            for (IntWritable val : values) {\n                sum += val.get();\n            }\n            context.write(key, new IntWritable(sum));\n        }\n    }\n\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n        Job job = new Job(conf,\"CountOverall\"); \n        job.setJarByClass(CountOverall.class);\n\n        job.setMapperClass(MyMap.class);\n        job.setCombinerClass(MyReduce.class);\n        job.setReducerClass(MyReduce.class);\n\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        FileInputFormat.addInputPath(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n\n    }\n\n}\n```\n\nprint_models.py\n\n```python\n\"\"\"\n * @author 简\n * 打印商品评级及各评级总数的柱状图.\n\"\"\"\nimport matplotlib.pyplot as plt\n\nfile = open(\"./KR/out/countOverall/part-r-00000\")\nx_list = []\ny_list = []\n\nline = file.readline()\nwhile line:\n    x = line.split()\n    x_list.append(x[0])\n    y_list.append(int(x[1]))\n    line = file.readline()\n\nfile.close()\n\nplt.bar(range(len(y_list)), y_list, width=0.8, facecolor=\"#4C72B0\", edgecolor=\"white\", tick_label=x_list)\nplt.xlabel('Level')\nplt.ylabel('Amount')\nplt.show()\n```\n\nCommentCountByDateOver20.java\n\n```java\n/**\n * @author 龚\n * 根据时间戳计算每天的评论数目，筛选出评论数目超过20条的记录.\n */\npublic class CommentCountByDateOver20 {\n\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n        String[] otherArgs = (new GenericOptionsParser(conf, args)).getRemainingArgs();\n        if (otherArgs.length < 2) {\n            System.err.println(\"Usage: Comments count by year <in> [<in>...] <out>\");\n            System.exit(2);\n        }\n        Job job = Job.getInstance(conf, \"CommentCountByDateOver20\");\n        job.setJarByClass(CommentCountByDateOver20.class);\n        job.setMapperClass(TokenizerMapper.class);\n        job.setCombinerClass(CountReducer.class);\n        job.setReducerClass(CountReducer.class);\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n        /*for (int i = 0; i < otherArgs.length - 1; ++i) {\n            FileInputFormat.addInputPath(job, new Path(otherArgs[i]));\n        }\n        FileOutputFormat.setOutputPath(job, new Path(otherArgs[otherArgs.length - 1]));*/\n        for (int i = 0; i < otherArgs.length - 1; ++i) {\n            FileInputFormat.addInputPath(job, new Path(otherArgs[i]));\n        }\n        FileOutputFormat.setOutputPath(job, new Path(otherArgs[otherArgs.length - 1]));\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n\n    public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {\n        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n            String line = value.toString();\n            Gson gson = new Gson();\n            KindleReview kindleReview = gson.fromJson(line, KindleReview.class);\n            Long unixReviewTime = kindleReview.getUnixReviewTime();\n            String reviewTime = new SimpleDateFormat(\"yyyy-MM-dd\").format(new Date(unixReviewTime * 1000));\n            context.write(new Text(reviewTime), new IntWritable(1));\n        }\n    }\n\n    public static class CountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n        public void reduce(Text key, Iterable<IntWritable> values,\n                           Context context\n        ) throws IOException, InterruptedException {\n            int sum = 0;\n            for (IntWritable value : values) {\n                sum += value.get();\n            }\n            if(sum > 20)\n                context.write(key, new IntWritable(sum));\n        }\n    }\n}\n```\n\nIsPaidPoster1.java\n\n```java\n/**\n * @author 简\n * 根据每人每天的评论条数筛选出潜在水军.\n */\npublic class IsPaidPoster1 {\n\n\tpublic static Map<String,Integer> rely = new HashMap<>();\n\t\n\tpublic static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {\n\n\t\tpublic void map(Object key, Text value, Context context) throws IOException,InterruptedException{\n\t\t\t\n\t\t\tString line = value.toString();\n            Gson gson = new Gson();\n            KindleReview kindleReview = gson.fromJson(line, KindleReview.class);\n            Long unixReviewTime = kindleReview.getUnixReviewTime();\n            String reviewTime = new SimpleDateFormat(\"yyyy-MM-dd\").format(new Date(unixReviewTime * 1000));\n            if(rely.containsKey(reviewTime)){\n                String reviewID = kindleReview.getReviewerID();\n                context.write(new Text(reviewTime + \",\" + reviewID), new IntWritable(1));\n            }\n\t\t}\n\t}\n\t\n\tpublic static class IntSumReducer extends Reducer<Text, IntWritable, Text, IntWritable>{\n        \n\t\tpublic void reduce(Text key,Iterable<IntWritable> values,Context context)throws IOException,InterruptedException{\n            \n            String str = key.toString();\n            String words[] = str.split(\",\");\n            int amount = rely.get(words[0]);\n            int sum = 0;\n            for (IntWritable value : values) {\n                sum += value.get();\n            }\n            if(sum > 15||sum > 0.25*amount)//某人某天评论数超过15条，或者，某人某天评论数超过评论总数的25%\n\t\t\t    context.write(key, new IntWritable(sum));\n\t\t}\n\t}\n\t\n\tpublic static void main(String[] args) throws Exception{\n\t\tConfiguration configuration=new Configuration();\n\t\tString[] otherArgs=new GenericOptionsParser(configuration,args).getRemainingArgs();\n\t\tif(otherArgs.length!=2){\n\t\t\tSystem.err.println(\"Usage:reverseIndex <in> <out>\");\n\t\t\tSystem.exit(2);\n\t\t}\n\t\tFileSystem fs=FileSystem.get(configuration);\n\t\tInputStream in = null;\n\t\tPath name1 = new Path(\"hdfs://localhost:9000/user/hadoop/commentCountByDateOver20/part-r-00000\");\n\t\tin = fs.open(name1);\n\t\tBufferedReader br = new BufferedReader(new InputStreamReader(in));\n\n\t\t//rely\n        String line = \"\";\n        line = br.readLine();\n\t\twhile (line != null) {\n            StringTokenizer itr = new StringTokenizer(line);\n            String str = itr.nextToken();\n\t\t\tint integer = Integer.parseInt(itr.nextToken());\n            rely.put(str, integer);\n\t\t\tline = br.readLine();\n\t\t}\n\t\tbr.close();\n        in.close();\n\t\t\t\t\n\t\t//配置作业名\n\t\t@SuppressWarnings(\"deprecation\")\n\t\tJob job=new Job(configuration, \"IsPaidPoster1\");\n\t\tjob.setJarByClass(IsPaidPoster1.class);\n\t\tjob.setMapperClass(TokenizerMapper.class);\n        //是可以按文件名进行partition的\n\t\tjob.setCombinerClass(IntSumReducer.class);\n\t\tjob.setReducerClass(IntSumReducer.class);\n\t\tjob.setOutputKeyClass(Text.class);\n\t\tjob.setOutputValueClass(IntWritable.class);\t\n        FileInputFormat.addInputPath(job, new Path(otherArgs[0]));\n\t\tFileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));\n\n        if(job.waitForCompletion(true)) {\n            InputStream in2 = null;\n            Path name2 = new Path(\"hdfs://localhost:9000/user/hadoop/\" +otherArgs[1]+ \"/part-r-00000\");\n            Path name3 = new Path(\"hdfs://localhost:9000/user/hadoop/\"+otherArgs[1]+\"/ans\");\t\t\n            in2 = fs.open(name2);\n            FSDataOutputStream fileout = fs.create(name3);\n\t        BufferedReader br2 = new BufferedReader(new InputStreamReader(in2));\n\t\t\tBufferedWriter out = new BufferedWriter(new OutputStreamWriter(fileout));\n\t        line = \"\";\n\t        line = br2.readLine();\n            Set<String> hs = new HashSet<String>();\n            int cnt = 0;\n\t        while (line != null) {\n                String words[] = line.split(\"[,\\\\p{Blank}]\");\n                if(!hs.contains(words[1])){\n                    cnt++;\n                    hs.add(words[1]);\n\t\t\t\t    out.write(words[1] + \"\\n\");\n                }\n\t            line = br2.readLine();\n\t        }\n            //out.write(\"\"+ cnt + \"\\n\");\n\t        br2.close();\n            in2.close();\n            out.flush();\n\t\t\tout.close();\n\t\t}\n\t\tSystem.exit(job.waitForCompletion(true)?0:1);\n\t}\n}\n```\n\nIsPaidPoster2.java\n\n```java\n/**\n * @author 简\n * 根据一个人所有的评论是否均为好评或恶评筛选出潜在水军.\n */\npublic class IsPaidPoster2 {\n\t\n\tpublic static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {\n\n\t\tpublic void map(Object key, Text value, Context context) throws IOException,InterruptedException{\n\t\t\t\n\t\t\tString line = value.toString();\n            Gson gson = new Gson();\n            KindleReview kindleReview = gson.fromJson(line, KindleReview.class);\n            String reviewID = kindleReview.getReviewerID();\n            int overall = kindleReview.getOverall();\n            context.write(new Text(reviewID), new IntWritable(overall));\n\t\t}\n\t}\n\t\n\tpublic static class IntSumReducer extends Reducer<Text, IntWritable, Text, Text>{\n        \n\t\tpublic void reduce(Text key,Iterable<IntWritable> values,Context context)throws IOException,InterruptedException{\n            \n            ArrayList<Integer> overall = new ArrayList<Integer>();\n            \n            float avg = 0;\n            for (IntWritable value : values) {\n                int val = value.get();\n                avg += val;\n                overall.add(val);\n            }\n            int cnt = overall.size();\n            if(cnt > 100){//当某个人的评论条数超过100条时\n                avg = avg/cnt;\n                if(avg < 1.1f||avg > 4.9f){//评级均为恶评或好评\n                    float sum = 0;\n                    for(int integer : overall){\n                        sum += (integer-avg)*(integer-avg);\n                    }\n                    sum = sum/cnt;\n                    if(sum < 0.1)//方差小于0.1\n\t\t                context.write(key, new Text(\"\" + avg + \" \" + sum));\n                }\n            }\n\t\t}\n\t}\n\t\n\tpublic static void main(String[] args) throws Exception{\n\t\tConfiguration configuration=new Configuration();\n\t\tString[] otherArgs=new GenericOptionsParser(configuration,args).getRemainingArgs();\n\t\tif(otherArgs.length!=2){\n\t\t\tSystem.err.println(\"Usage:reverseIndex <in> <out>\");\n\t\t\tSystem.exit(2);\n\t\t}\n\t\tFileSystem fs=FileSystem.get(configuration);\n\t\t\t\t\n\t\t//配置作业名\n\t\t@SuppressWarnings(\"deprecation\")\n\t\tJob job=new Job(configuration, \"IsPaidPoster2\");\n\t\tjob.setJarByClass(IsPaidPoster2.class);\n\t\tjob.setMapperClass(TokenizerMapper.class);\n\t\tjob.setReducerClass(IntSumReducer.class);\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(IntWritable.class);\n\t\tjob.setOutputKeyClass(Text.class);\n\t\tjob.setOutputValueClass(Text.class);\t\n        FileInputFormat.addInputPath(job, new Path(otherArgs[0]));\n\t\tFileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));\n\n        if(job.waitForCompletion(true)) {\n            InputStream in2 = null;\n            Path name2 = new Path(\"hdfs://localhost:9000/user/hadoop/\" +otherArgs[1]+ \"/part-r-00000\");\n            Path name3 = new Path(\"hdfs://localhost:9000/user/hadoop/\"+otherArgs[1]+\"/ans\");\t\t\n            in2 = fs.open(name2);\n            FSDataOutputStream fileout = fs.create(name3);\n\t        BufferedReader br2 = new BufferedReader(new InputStreamReader(in2));\n\t\t\tBufferedWriter out = new BufferedWriter(new OutputStreamWriter(fileout));\n\n            InputStream in1 = null;\n            Path name1 = new Path(\"hdfs://localhost:9000/user/hadoop/isPaidPoster1/ans\");\t\n            in1 = fs.open(name1);\n\t        BufferedReader br1 = new BufferedReader(new InputStreamReader(in1));\n\n\t        String line = \"\";\n            Set<String> hs = new HashSet<String>();\n            int cnt = 0;\n            line = br1.readLine();\n\t        while (line != null) {\n                cnt++;\n                hs.add(line);\n\t\t\t    out.write(line + \"\\n\");\n\t            line = br1.readLine();\n\t        }\n            line = br2.readLine();\n\t        while (line != null) {\n                String words[] = line.split(\"[\\\\p{Blank}]\");\n                if(!hs.contains(words[0])){\n                    cnt++;\n\t\t\t        out.write(words[0] + \"\\n\");\n                }\n\t            line = br2.readLine();\n\t        }\n            //out.write(\"\"+ cnt + \"\\n\");\n            br1.close();\n            in1.close();\n\t        br2.close();\n            in2.close();\n            out.flush();\n\t\t\tout.close();\n\t\t}\n\t\tSystem.exit(job.waitForCompletion(true)?0:1);\n\t}\n}\n```\n\nOverallClean.java\n\n```java\n/**\n * @author 杨\n * 剔除水军，重新计算各商品平均评级.\n */\npublic class OverallClean{\n\n    public static Set<String> rely = new HashSet<String>();\n\n    public static class MyMap extends Mapper<Object, Text, Text, IntWritable> {\n    \n        public void map(Object key, Text value, Context context) \n                throws IOException, InterruptedException {\n            \n            String review = value.toString();\n            Gson gson = new Gson();\n            KindleReview kindleReview = gson.fromJson(review, KindleReview.class);\n            int overall = kindleReview.getOverall();\n            String asin = kindleReview.getAsin();\n            \n            String reviewId = kindleReview.getReviewerID();\n            if(!rely.contains(reviewId))//去除不良评判人员\n\t            context.write(new Text(asin), new IntWritable(overall));            \n        }\n    }\n\n  \n    public static class MyReduce extends\n            Reducer<Text,IntWritable, Text, Text> {\n\n        public void reduce(Text key, Iterable<IntWritable> values,\n                Context context) throws IOException, InterruptedException {\n            int sum = 0;\n\t        int i=0;\n\t        float ave=0;\n            for (IntWritable val : values) {\n                sum += val.get();\n\t\t        i++;\n            }\n\t        float s=(float)sum;\n\t        float j=(float)i;\n\t        ave=s/j;\n            context.write(key, new Text(\"\" + ave));\n        }\n    }\n\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n\n        FileSystem fs=FileSystem.get(conf);\n\t\tInputStream in = null;\n\t\tPath name = new Path(\"hdfs://localhost:9000/user/hadoop/isPaidPoster2/ans\");\n\t\tin = fs.open(name);\n\t\tBufferedReader br = new BufferedReader(new InputStreamReader(in));\n\n\t\t//rely\n        String line = \"\";\n        line = br.readLine();\n\t\twhile (line != null) {\n            rely.add(line);\n\t\t\tline = br.readLine();\n\t\t}\n\t\tbr.close();\n        in.close();\n\n        Job job = new Job(conf,\"OverallClean\"); \n        job.setJarByClass(OverallClean.class);\t\n        job.setMapperClass(MyMap.class);\n        job.setReducerClass(MyReduce.class);\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(Text.class);\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(IntWritable.class);\n        FileInputFormat.addInputPath(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        if(job.waitForCompletion(true)) {\n            InputStream in2 = null;\n            Path name2 = new Path(\"hdfs://localhost:9000/user/hadoop/\" +args[1]+ \"/part-r-00000\");\n            Path name3 = new Path(\"hdfs://localhost:9000/user/hadoop/\"+args[1]+\"/ans\");\t\t\n            in2 = fs.open(name2);\n            FSDataOutputStream fileout = fs.create(name3);\n\t        BufferedReader br2 = new BufferedReader(new InputStreamReader(in2));\n\t\t\tBufferedWriter out = new BufferedWriter(new OutputStreamWriter(fileout));\n\n            InputStream in1 = null;\n            Path name1 = new Path(\"hdfs://localhost:9000/user/hadoop/overall/part-r-00000\");\t\n            in1 = fs.open(name1);\n\t        BufferedReader br1 = new BufferedReader(new InputStreamReader(in1));\n\n\t        line = \"\";\n            Set<String> hs = new HashSet<String>();\n            int cnt = 0;\n            line = br2.readLine();\n\t        while (line != null) {\n                String words[] = line.split(\"[\\\\p{Blank}]\");\n                hs.add(words[0]);\n\t            line = br2.readLine();\n\t        }\n            out.write(\"Lack...\\n\");\n            line = br1.readLine();\n\t        while (line != null) {\n                String words[] = line.split(\"[\\\\p{Blank}]\");\n                if(!hs.contains(words[0])){\n                    cnt++;\n\t\t\t        out.write(words[0] + \" \" + words[1] + \"\\n\");\n                }\n\t            line = br1.readLine();\n\t        }\n            out.write(\"\"+ cnt + \"\\n\");\n            br1.close();\n            in1.close();\n\t        br2.close();\n            in2.close();\n            out.flush();\n\t\t\tout.close();\n\t\t}\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n}\n```\n\nOverallWeights.java\n\n```java\n/**\n * @author 简\n * 根据帮助度和时效性，更改同一商品每条评论的评级权重，重新计算剔除水军\n * 评论后的各商品平均评级.\n */\npublic class OverallWeights {\n    \n    public static Set<String> rely = new HashSet<String>();\n\t\n\tpublic static class TokenizerMapper extends Mapper<Object, Text, Text, Text> {\n\n\t\tpublic void map(Object key, Text value, Context context) throws IOException,InterruptedException{\n\t\t\t\n\t\t\tString line = value.toString();\n            Gson gson = new Gson();\n            KindleReview kindleReview = gson.fromJson(line, KindleReview.class);\n            String asin = kindleReview.getAsin();\n            int overall = kindleReview.getOverall();\n            String[] help = kindleReview.getHelpful();\n            Long unixReviewTime = kindleReview.getUnixReviewTime();\n\n            String reviewId = kindleReview.getReviewerID();\n            if(!rely.contains(reviewId))//去除不良评判人员\t\t\n                context.write(new Text(asin), new Text(overall + \",\" + help[0] + \",\" + help [1] + \",\" + unixReviewTime));\n\t\t}\n\t}\n\t\n\tpublic static class IntSumReducer extends Reducer<Text, Text, Text, Text>{\n        \n\t\tpublic void reduce(Text key,Iterable<Text> values,Context context)throws IOException,InterruptedException{\n            \n            int a = 1, b = 1;//帮助度和时效性的比例为1：1\n            ArrayList<Integer> overall = new ArrayList<Integer>();\n            ArrayList<Float> help = new ArrayList<Float>();\n            ArrayList<Long> unixTime = new ArrayList<Long>();\n            \n            for (Text value : values) {\n                String str = value.toString();\n                String words[] = str.split(\",\");\n                int left = Integer.parseInt(words[1]);\n                int right = Integer.parseInt(words[2]);\n                float ans = 0;\n                if(right != 0)\n                    ans = (float)left/right;\n                \n                overall.add(Integer.parseInt(words[0]));\n                help.add(ans);\n                unixTime.add(Long.parseLong(words[3]));\n            }\n\n            //帮助度\n            float sum = 0, ans = 0.5f;//当一个产品没有任何帮助度相关信息时，设置帮助度为0.5\n            int cnt = 0;\n            for(float f : help){\n                if(f > 0.00001f){\n                    sum += f;\n                    cnt++;\n                }\n            }\n            if(cnt != 0)\n                ans = sum/cnt;\n            Collections.replaceAll(help, 0.0f, ans);\n            //时效性\n            long newTime = 1406073600;\n\n            //平均评级\n            float helpRate = 0;\n            double s1 = 0;\n            double s2 = 0;\n            int all = overall.size();\n            for(int i=0; i<all; i++){\n                float timeRate = (float)unixTime.get(i);\n                helpRate = help.get(i)*a + timeRate*timeRate/newTime*b;\n                s1 += (double)helpRate*overall.get(i);\n                s2 += (double)helpRate;\n            }\n\t\t    context.write(key, new Text(\"\" + s1/s2));\n        }\n\t}\n\t\n\t\n\tpublic static void main(String[] args) throws Exception{\n\t\tConfiguration configuration=new Configuration();\n\t\tString[] otherArgs=new GenericOptionsParser(configuration,args).getRemainingArgs();\n\t\tif(otherArgs.length!=2){\n\t\t\tSystem.err.println(\"Usage:reverseIndex <in> <out>\");\n\t\t\tSystem.exit(2);\n\t\t}\n\n        FileSystem fs=FileSystem.get(configuration);\n\t\tInputStream in = null;\n\t\tPath name = new Path(\"hdfs://localhost:9000/user/hadoop/isPaidPoster2/ans\");\n\t\tin = fs.open(name);\n\t\tBufferedReader br = new BufferedReader(new InputStreamReader(in));\n\n\t\t//rely\n        String line = \"\";\n        line = br.readLine();\n\t\twhile (line != null) {\n            rely.add(line);\n\t\t\tline = br.readLine();\n\t\t}\n\t\tbr.close();\n        in.close();\n\t\t\t\t\n\t\t//配置作业名\n\t\t@SuppressWarnings(\"deprecation\")\n\t\tJob job=new Job(configuration, \"OverallWeights\");\n\t\tjob.setJarByClass(OverallWeights.class);\n\t\tjob.setMapperClass(TokenizerMapper.class);\n\t\tjob.setReducerClass(IntSumReducer.class);\n\t\tjob.setOutputKeyClass(Text.class);\n\t\tjob.setOutputValueClass(Text.class);\t\n        FileInputFormat.addInputPath(job, new Path(otherArgs[0]));\n\t\tFileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));\n\n\t\tSystem.exit(job.waitForCompletion(true)?0:1);\n\t}\n}\n\n```\n\nprint_zhe.py\n\n```python\n\"\"\"\n * @author 简\n * 绘制三种处理方式后的对比折线图.\n\"\"\"\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nf1 = open(\"./pic/overall\")\nf2 = open(\"./pic/overallClean\")\nf3 = open(\"./pic/overallWeights\")\nx1 = []\nx2 = []\nx3 = []\ny1 = []\ny2 = []\ny3 = []\n\nline = f1.readline()\nwhile line:\n    x = line.split()\n    x1.append(x[0])\n    #y1.append(int(x[1]))\n    y1.append(float(x[1]))\n    line = f1.readline()\nline = f2.readline()\nwhile line:\n    x = line.split()\n    x2.append(x[0])\n    y2.append(float(x[1]))\n    line = f2.readline()\nline = f3.readline()\nwhile line:\n    x = line.split()\n    x3.append(x[0])\n    y3.append(float(x[1]))\n    line = f3.readline()\n\nx=np.arange(100,350)\nplt.figure(figsize=(10, 10))\nl1=plt.plot(x1,y1,'r--',label='avgReviewOverall')\nl2=plt.plot(x2,y2,'b--',label='avgReviewOverallClean')\nl3=plt.plot(x3,y3,'g--',label='avgReviewOverallWeights')\nplt.plot(x1,y1,'ro-',x2,y2,'b+-',x3,y3,'g^-')\nplt.title('Average number of comments for each rated product')\nplt.xlabel('Rating')\nplt.ylabel('Average number of comments')\nplt.legend()\n#显示出所有设置\nplt.show()\n```\n\nAvgReview.java\n\n```java\n/**\n * @author 简\n * 计算各评级阶段的商品拥有的平均评论条数.\n */\npublic class AvgReview{\n\n    public static Map<String,Integer> rely1 = new HashMap<>();\n    public static Map<String,Integer> rely2 = new HashMap<>();\n    public static Set<String> rely3 = new HashSet<String>();\n\n    public static class MyMap extends Mapper<Object, Text, Text, IntWritable> {\n    \n        public void map(Object key, Text value, Context context) \n                throws IOException, InterruptedException {\n            \n            String review = value.toString();\n            Gson gson = new Gson();\n            KindleReview kindleReview = gson.fromJson(review, KindleReview.class);\n            String asin = kindleReview.getAsin();\n            String reviewId = kindleReview.getReviewerID();\n            if(!rely3.contains(reviewId)&&rely1.containsKey(asin)){\n                int overall = rely1.get(asin);\n                context.write(new Text(\"\" + overall), new IntWritable(1));\n            }\n        }\n    }\n\n  \n    public static class MyReduce extends\n            Reducer<Text,IntWritable, Text, Text> {\n\n        public void reduce(Text key, Iterable<IntWritable> values,\n                Context context) throws IOException, InterruptedException {\n            String overall = key.toString();\n\t        int i=0;\n            for (IntWritable val : values) {\n\t\t        i++;\n            }\n            context.write(key, new Text(\"\" + (float)i/rely2.get(overall)));\n            //context.write(key, new Text(\"\" + i));\n        }\n    }\n\n    public static void main(String[] args) throws Exception {\n        //输入参数：KR评论json文件 商品评级 每种评级的商品种数 输出文件\n        Configuration conf = new Configuration();\n\n        FileSystem fs=FileSystem.get(conf);\n\t\tInputStream in1 = null;\n        InputStream in2 = null;\n\t\tInputStream in3 = null;\n\t\tPath name1 = new Path(\"hdfs://localhost:9000/user/hadoop/\"+args[1]+\"/part-r-00000\");\n\t\tPath name2 = new Path(\"hdfs://localhost:9000/user/hadoop/\"+args[2]+\"/part-r-00000\");\n\t\tPath name3 = new Path(\"hdfs://localhost:9000/user/hadoop/isPaidPoster2/ans\");\n\t\tin1 = fs.open(name1);\n\t\tin2 = fs.open(name2);\n\t\tin3 = fs.open(name3);\n\t\tBufferedReader br1 = new BufferedReader(new InputStreamReader(in1));\n\t\tBufferedReader br2 = new BufferedReader(new InputStreamReader(in2));\n        BufferedReader br3 = new BufferedReader(new InputStreamReader(in3));\n\n\t\t//rely\n        String line = \"\";\n        line = br1.readLine();\n\t\twhile (line != null) {\n            StringTokenizer itr = new StringTokenizer(line);\n            String str = itr.nextToken();\n\t\t\tint integer = (int)Double.parseDouble(itr.nextToken());\n            rely1.put(str, integer);\n\t\t\tline = br1.readLine();\n\t\t}\n        line = br2.readLine();\n\t\twhile (line != null) {\n            StringTokenizer itr = new StringTokenizer(line);\n            String str = itr.nextToken();\n\t\t\tint integer = Integer.parseInt(itr.nextToken());\n            rely2.put(str, integer);\n\t\t\tline = br2.readLine();\n\t\t}\n        line = br3.readLine();\n\t\twhile (line != null) {\n            rely3.add(line);\n\t\t\tline = br3.readLine();\n\t\t}\n\t\tbr1.close();\n        br2.close();\n        br3.close();\n        in1.close();\n        in2.close();\n        in3.close();\n\n        Job job = new Job(conf,\"AvgReview\"); \n        job.setJarByClass(AvgReview.class);\t\n        job.setMapperClass(MyMap.class);\n        job.setReducerClass(MyReduce.class);\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(Text.class);\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(IntWritable.class);\n        FileInputFormat.addInputPath(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[3]));\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n}\n```\n\nAvgReview.java\n\n```java\n/**\n * @author 简\n * 计算各评级阶段的商品拥有的平均评论条数.\n */\npublic class AvgReview{\n\n    public static Map<String,Integer> rely1 = new HashMap<>();\n    public static Map<String,Integer> rely2 = new HashMap<>();\n    public static Set<String> rely3 = new HashSet<String>();\n\n    public static class MyMap extends Mapper<Object, Text, Text, IntWritable> {\n    \n        public void map(Object key, Text value, Context context) \n                throws IOException, InterruptedException {\n            \n            String review = value.toString();\n            Gson gson = new Gson();\n            KindleReview kindleReview = gson.fromJson(review, KindleReview.class);\n            String asin = kindleReview.getAsin();\n            String reviewId = kindleReview.getReviewerID();\n            if(!rely3.contains(reviewId)&&rely1.containsKey(asin)){\n                int overall = rely1.get(asin);\n                context.write(new Text(\"\" + overall), new IntWritable(1));\n            }\n        }\n    }\n\n  \n    public static class MyReduce extends\n            Reducer<Text,IntWritable, Text, Text> {\n\n        public void reduce(Text key, Iterable<IntWritable> values,\n                Context context) throws IOException, InterruptedException {\n            String overall = key.toString();\n\t        int i=0;\n            for (IntWritable val : values) {\n\t\t        i++;\n            }\n            context.write(key, new Text(\"\" + (float)i/rely2.get(overall)));\n            //context.write(key, new Text(\"\" + i));\n        }\n    }\n\n    public static void main(String[] args) throws Exception {\n        //输入参数：KR评论json文件 商品评级 每种评级的商品种数 输出文件\n        Configuration conf = new Configuration();\n\n        FileSystem fs=FileSystem.get(conf);\n\t\tInputStream in1 = null;\n        InputStream in2 = null;\n\t\tInputStream in3 = null;\n\t\tPath name1 = new Path(\"hdfs://localhost:9000/user/hadoop/\"+args[1]+\"/part-r-00000\");\n\t\tPath name2 = new Path(\"hdfs://localhost:9000/user/hadoop/\"+args[2]+\"/part-r-00000\");\n\t\tPath name3 = new Path(\"hdfs://localhost:9000/user/hadoop/isPaidPoster2/ans\");\n\t\tin1 = fs.open(name1);\n\t\tin2 = fs.open(name2);\n\t\tin3 = fs.open(name3);\n\t\tBufferedReader br1 = new BufferedReader(new InputStreamReader(in1));\n\t\tBufferedReader br2 = new BufferedReader(new InputStreamReader(in2));\n        BufferedReader br3 = new BufferedReader(new InputStreamReader(in3));\n\n\t\t//rely\n        String line = \"\";\n        line = br1.readLine();\n\t\twhile (line != null) {\n            StringTokenizer itr = new StringTokenizer(line);\n            String str = itr.nextToken();\n\t\t\tint integer = (int)Double.parseDouble(itr.nextToken());\n            rely1.put(str, integer);\n\t\t\tline = br1.readLine();\n\t\t}\n        line = br2.readLine();\n\t\twhile (line != null) {\n            StringTokenizer itr = new StringTokenizer(line);\n            String str = itr.nextToken();\n\t\t\tint integer = Integer.parseInt(itr.nextToken());\n            rely2.put(str, integer);\n\t\t\tline = br2.readLine();\n\t\t}\n        line = br3.readLine();\n\t\twhile (line != null) {\n            rely3.add(line);\n\t\t\tline = br3.readLine();\n\t\t}\n\t\tbr1.close();\n        br2.close();\n        br3.close();\n        in1.close();\n        in2.close();\n        in3.close();\n\n        Job job = new Job(conf,\"AvgReview\"); \n        job.setJarByClass(AvgReview.class);\t\n        job.setMapperClass(MyMap.class);\n        job.setReducerClass(MyReduce.class);\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(Text.class);\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(IntWritable.class);\n        FileInputFormat.addInputPath(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[3]));\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n}\n```\n\nSentimentAnalysis.java\n\n```java\n/**\n * @author 龚\n * 情绪分析.\n */\npublic class SentimentAnalysis {\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n        String[] otherArgs = (new GenericOptionsParser(conf, args)).getRemainingArgs();\n        if (otherArgs.length < 2) {\n            System.err.println(\"Usage: SentimentAnalysis <in> [<in>...] <out>\");\n            System.exit(2);\n        }\n        Job job = Job.getInstance(conf, \"SentimentAnalysis\");\n        job.setJarByClass(SentimentAnalysis.class);\n        job.setMapperClass(TokenizerMapper.class);\n        job.setCombinerClass(IReducer.class);\n        job.setReducerClass(IReducer.class);\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n        for (int i = 0; i < otherArgs.length - 1; ++i) {\n            FileInputFormat.addInputPath(job, new Path(otherArgs[i]));\n        }\n        FileOutputFormat.setOutputPath(job, new Path(otherArgs[otherArgs.length - 1]));\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n\n    public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {\n        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n            String line = value.toString();\n            Gson gson = new Gson();\n            KindleReview kindleReview = gson.fromJson(line, KindleReview.class);\n            String ReviewText = kindleReview.getReviewText();\n            Properties props = new Properties();\n            props.put(\"annotators\", \"tokenize, ssplit, pos, parse, sentiment\");\n            props.setProperty(\"parse.model\", \"edu/stanford/nlp/models/srparser/englishSR.ser.gz\");\n            StanfordCoreNLP pipeline = new StanfordCoreNLP(props);\n            int mainSentiment = 0;\n            if (ReviewText != null && ReviewText.length() > 0) {\n                int longest = 0;\n                Annotation annotation = pipeline.process(ReviewText);\n                for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {\n                    Tree tree = sentence.get(SentimentCoreAnnotations.SentimentAnnotatedTree.class);\n                    int sentiment = RNNCoreAnnotations.getPredictedClass(tree);\n                    String partText = sentence.toString();\n                    if (partText.length() > longest) {\n                        mainSentiment = sentiment;\n                        longest = partText.length();\n                    }\n                }\n            }\n            context.write(new Text(kindleReview.getAsin() + \" \" + kindleReview.getReviewerID()), new IntWritable(mainSentiment + 1));\n        }\n    }\n\n    public static class IReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n        public void reduce(Text key, Iterable<IntWritable> values,\n                           Context context\n        ) throws IOException, InterruptedException {\n            for (IntWritable value : values) {\n                context.write(key, value);\n            }\n        }\n    }\n\n}\n```\n\nOverallSentiment.java\n\n```java\n/**\n * @author 简\n * 计算经过剔除水军、修改权重、情绪分析后的商品评级.\n */\npublic class OverallSentiment {\n    \n    public static Set<String> rely = new HashSet<String>();\n\tpublic static Map<String,Integer> map = new HashMap<>();\n\t\n\tpublic static class TokenizerMapper extends Mapper<Object, Text, Text, Text> {\n\n\t\tpublic void map(Object key, Text value, Context context) throws IOException,InterruptedException{\n\t\t\t\n\t\t\tString line = value.toString();\n            Gson gson = new Gson();\n            KindleReview kindleReview = gson.fromJson(line, KindleReview.class);\n            String asin = kindleReview.getAsin();\n            float overall = kindleReview.getOverall();\n            String[] help = kindleReview.getHelpful();\n            Long unixReviewTime = kindleReview.getUnixReviewTime();\n\n            String reviewId = kindleReview.getReviewerID();\n            if(!rely.contains(reviewId)){//去除不良评判人员\n                overall = (float)(overall+map.get(asin + \" \" + reviewId))/2;\n                context.write(new Text(asin), new Text(overall + \",\" + help[0] + \",\" + help [1] + \",\" + unixReviewTime));\n            }\n\t\t}\n\t}\n\t\n\tpublic static class IntSumReducer extends Reducer<Text, Text, Text, Text>{\n        \n\t\tpublic void reduce(Text key,Iterable<Text> values,Context context)throws IOException,InterruptedException{\n            \n            int a = 1, b = 1;//帮助度和时效性的比例为1：1\n            ArrayList<Float> overall = new ArrayList<Float>();\n            ArrayList<Float> help = new ArrayList<Float>();\n            ArrayList<Long> unixTime = new ArrayList<Long>();\n            \n            for (Text value : values) {\n                String str = value.toString();\n                String words[] = str.split(\",\");\n                int left = Integer.parseInt(words[1]);\n                int right = Integer.parseInt(words[2]);\n                float ans = 0;\n                if(right != 0)\n                    ans = (float)left/right;\n                \n                overall.add(Float.parseFloat(words[0]));\n                help.add(ans);\n                unixTime.add(Long.parseLong(words[3]));\n            }\n\n            //帮助度\n            float sum = 0, ans = 0.5f;//当一个产品没有任何帮助度相关信息时，设置帮助度为0.5\n            int cnt = 0;\n            for(float f : help){\n                if(f > 0.00001f){\n                    sum += f;\n                    cnt++;\n                }\n            }\n            if(cnt != 0)\n                ans = sum/cnt;\n            Collections.replaceAll(help, 0.0f, ans);\n            //时效性\n            long newTime = 1406073600;\n\n            //平均评级\n            float helpRate = 0;\n            double s1 = 0;\n            double s2 = 0;\n            int all = overall.size();\n            for(int i=0; i<all; i++){\n                float timeRate = (float)unixTime.get(i);\n                helpRate = help.get(i)*a + timeRate*timeRate/newTime*b;\n                s1 += (double)helpRate*overall.get(i);\n                s2 += (double)helpRate;\n            }\n\t\t    context.write(key, new Text(\"\" + s1/s2));\n        }\n\t}\n\t\n\t\n\tpublic static void main(String[] args) throws Exception{\n\t\tConfiguration configuration=new Configuration();\n\t\tString[] otherArgs=new GenericOptionsParser(configuration,args).getRemainingArgs();\n\t\tif(otherArgs.length!=2){\n\t\t\tSystem.err.println(\"Usage:reverseIndex <in> <out>\");\n\t\t\tSystem.exit(2);\n\t\t}\n\n        FileSystem fs=FileSystem.get(configuration);\n\t\tInputStream in1 = null;\n        InputStream in2 = null;\n\t\tPath name1 = new Path(\"hdfs://localhost:9000/user/hadoop/isPaidPoster2/ans\");\n\t\tPath name2 = new Path(\"hdfs://localhost:9000/user/hadoop/sentiment/sentiment_analysis\");\n\t\tin1 = fs.open(name1);\n\t\tin2 = fs.open(name2);\n\t\tBufferedReader br1 = new BufferedReader(new InputStreamReader(in1));\t\t\n\t\tBufferedReader br2 = new BufferedReader(new InputStreamReader(in2));\n\n\t\t//rely\n        String line = \"\";\n        line = br1.readLine();\n\t\twhile (line != null) {\n            rely.add(line);\n\t\t\tline = br1.readLine();\n\t\t}\n        line = br2.readLine();\n\t\twhile (line != null) {\n            StringTokenizer itr = new StringTokenizer(line);\n            String asin = itr.nextToken();\n\t\t\tString reviewId = itr.nextToken();\n\t\t\tint integer = (int)Double.parseDouble(itr.nextToken());\n            map.put(asin + \" \" + reviewId, integer);\n\t\t\tline = br2.readLine();\n\t\t}\n\t\tbr1.close();\n        br2.close();\n        in1.close();\n        in2.close();\n\t\t\t\t\n\t\t//配置作业名\n\t\t@SuppressWarnings(\"deprecation\")\n\t\tJob job=new Job(configuration, \"OverallSentiment\");\n\t\tjob.setJarByClass(OverallSentiment.class);\n\t\tjob.setMapperClass(TokenizerMapper.class);\n\t\tjob.setReducerClass(IntSumReducer.class);\n\t\tjob.setOutputKeyClass(Text.class);\n\t\tjob.setOutputValueClass(Text.class);\t\n        FileInputFormat.addInputPath(job, new Path(otherArgs[0]));\n\t\tFileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));\n\n\t\tSystem.exit(job.waitForCompletion(true)?0:1);\n\t}\n}\n```\n\nKindleReview.java\n\n```java\n/**\n * @author 龚\n * json格式数据处理类.\n */\nimport java.util.Arrays;\n\npublic class KindleReview {\n    String reviewerID;\n    String asin;\n    String reviewerName;\n    String[] helpful;\n    String reviewText;\n    int overall;\n    String summary;\n    long unixReviewTime;\n    String reviewTime;\n\n    public String getReviewerID() {\n        return reviewerID;\n    }\n\n    public void setReviewerID(String reviewerID) {\n        this.reviewerID = reviewerID;\n    }\n\n    public String getAsin() {\n        return asin;\n    }\n\n    public void setAsin(String asin) {\n        this.asin = asin;\n    }\n\n    public String getReviewerName() {\n        return reviewerName;\n    }\n\n    public void setReviewerName(String reviewerName) {\n        this.reviewerName = reviewerName;\n    }\n\n    public String[] getHelpful() {\n        return helpful;\n    }\n\n    public void setHelpful(String[] helpful) {\n        this.helpful = helpful;\n    }\n\n    public String getReviewText() {\n        return reviewText;\n    }\n\n    public void setReviewText(String reviewText) {\n        this.reviewText = reviewText;\n    }\n\n    public int getOverall() {\n        return overall;\n    }\n\n    public void setOverall(int overall) {\n        this.overall = overall;\n    }\n\n    public String getSummary() {\n        return summary;\n    }\n\n    public void setSummary(String summary) {\n        this.summary = summary;\n    }\n\n    public long getUnixReviewTime() {\n        return unixReviewTime;\n    }\n\n    public void setUnixReviewTime(long unixReviewTime) {\n        this.unixReviewTime = unixReviewTime;\n    }\n\n    public String getReviewTime() {\n        return reviewTime;\n    }\n\n    public void setReviewTime(String reviewTime) {\n        this.reviewTime = reviewTime;\n    }\n\n    @Override\n    public String toString() {\n        return \"KindleReview{\" +\n                \"reviewerID='\" + reviewerID + '\\'' +\n                \", asin='\" + asin + '\\'' +\n                \", reviewerName='\" + reviewerName + '\\'' +\n                \", helpful=\" + Arrays.toString(helpful) +\n                \", reviewText='\" + reviewText + '\\'' +\n                \", overall=\" + overall +\n                \", summary='\" + summary + '\\'' +\n                \", unixReviewTime=\" + unixReviewTime +\n                \", reviewTime='\" + reviewTime + '\\'' +\n                '}';\n    }\n}\n```\n\n ","tags":["专业课"]},{"title":"Markdown公式合集","url":"/Markdown公式合集/","content":"\n## 常用公式符号\n\n### 角标\n\n\"_\"—>down\n\"^\"—>up\n\n```\nx_1 \\\\\\\\\n\nx_1^2 \\\\\\\\\n\nx^2_1 \\\\\n\nx_{22}^{(n)} \\\\\n\n^*x^* \\\\\n\nx_{down}^{up}\n```\n\n\n$$\nx_1 \\\\\\\\\n\nx_1^2 \\\\\\\\\n\nx^2_1 \\\\\n\nx_{22}^{(n)} \\\\\n\n^*x^* \\\\\n\nx_{down}^{up}\n$$\n\n### 分号\n\nfrac\n\n```\n\\frac{x+y}{2} \\\\\n\n\\frac{1}{1+\\frac{1}{2}}\n```\n\n\n$$\n\\frac{x+y}{2} \\\\\n\n\\frac{1}{1+\\frac{1}{2}}\n$$\n\n### 根号\n\nsqrt\n第二和第三个的区别在于为了美观微调位置 ^_^\n\n```\n\\sqrt{2}<\\sqrt[3]{3} \\\\\n\n\\sqrt{1+\\sqrt[p]{1+a^2}} \\\\\n\n\\sqrt{1+\\sqrt[^p\\!]{1+a^2}}\n```\n\n\n$$\n\\sqrt{2}<\\sqrt[3]{3} \\\\\n\n\\sqrt{1+\\sqrt[p]{1+a^2}} \\\\\n\n\\sqrt{1+\\sqrt[^p\\!]{1+a^2}}\n$$\n\n### 求和、积分\n\nsum\nint\n\n```\n\\sum_{k=1}^{n}\\frac{1}{k} \\\\\n\n\\int_a^b f(x)dx \\\\\n\n\\int_a^b f(x)\\mathrm{d}x\n```\n\n$$\n\\sum_{k=1}^{n}\\frac{1}{k} \\\\\n\n\\int_a^b f(x)dx \\\\\n\n\\int_a^b f(x)\\mathrm{d}x\n$$\n\n### 空格\n\n```\n紧贴 a\\!b \\\\\n没有空格 ab \\\\\n小空格 a\\,b \\\\\n中等空格 a\\;b \\\\\n大空格 a\\ b \\\\\nquad空格 a\\quad b \\\\\n两个quad空格 a\\qquad b \\\\\n```\n\n\n$$\n紧贴 a\\!b \\\\\n没有空格 ab \\\\\n小空格 a\\,b \\\\\n中等空格 a\\;b \\\\\n大空格 a\\ b \\\\\nquad空格 a\\quad b \\\\\n两个quad空格 a\\qquad b \\\\\n$$\n\n### 括号\n\n```\n\\left(\\sum_{k=\\frac{1}{2}}^{N^2}\\frac{1}{k}\\right) \\\\\n(\\sum_{k=\\frac{1}{2}}^{N^2}\\frac{1}{k})\n```\n\n\n$$\n\\left(\\sum_{k=\\frac{1}{2}}^{N^2}\\frac{1}{k}\\right) \\\\\n(\\sum_{k=\\frac{1}{2}}^{N^2}\\frac{1}{k})\n$$\n\n### 矩阵\n\n```\n\\begin{matrix}1&2\\\\3&4\\end{matrix} \\\\\n\n\\begin{pmatrix}1 & 2\\\\\\\\3 &4\\end{pmatrix} \\\\\n\n\\begin{bmatrix}1 & 2\\\\\\\\3 &4\\end{bmatrix} \\\\\n\n\\begin{Bmatrix}1 & 2\\\\\\\\3 &4\\end{Bmatrix} \\\\\n\n\\begin{vmatrix}1 & 2\\\\\\\\3 &4\\end{vmatrix} \\\\\n\n\\left|\\begin{matrix}1 & 2\\\\\\\\3 &4\\end{matrix}\\right| \\\\\n\n\\begin{Vmatrix}1 & 2\\\\\\\\3 &4\\end{Vmatrix}\n```\n\n\n$$\n\\begin{matrix}1&2\\\\3&4\\end{matrix} \\\\\n\n\\begin{pmatrix}1 & 2\\\\\\\\3 &4\\end{pmatrix} \\\\\n\n\\begin{bmatrix}1 & 2\\\\\\\\3 &4\\end{bmatrix} \\\\\n\n\\begin{Bmatrix}1 & 2\\\\\\\\3 &4\\end{Bmatrix} \\\\\n\n\\begin{vmatrix}1 & 2\\\\\\\\3 &4\\end{vmatrix} \\\\\n\n\\left|\\begin{matrix}1 & 2\\\\\\\\3 &4\\end{matrix}\\right| \\\\\n\n\\begin{Vmatrix}1 & 2\\\\\\\\3 &4\\end{Vmatrix}\n$$\n\n```\n\\mathbf{X} =\n\\left( \\begin{array}{ccc}\nx_{11} & x\\_{12} & \\ldots \\\\\\\\\nx\\_{21} & x\\_{22} & \\ldots \\\\\\\\\n\\vdots & \\vdots & \\ddots\n\\end{array} \\right)\n```\n\n\n$$\n\\mathbf{X} =\n\\left( \\begin{array}{ccc}\nx_{11} & x\\_{12} & \\ldots \\\\\\\\\nx\\_{21} & x\\_{22} & \\ldots \\\\\\\\\n\\vdots & \\vdots & \\ddots\n\\end{array} \\right)\n$$\n\n### 长公式\n\n```\n不对齐\\\\\n\\begin{multline}\nx = a+b+c+{} \\\\\\\\\nd+e+f+g\n\\end{multline}\n\\\\对齐\\\\\n\\begin{aligned}\nx ={}& a+b+c+{} \\\\\\\\\n&d+e+f+g\n\\end{aligned}\n```\n\n\n$$\n不对齐\\\\\n\\begin{multline}\nx = a+b+c+{} \\\\\\\\\nd+e+f+g\n\\end{multline}\n\\\\对齐\\\\\n\\begin{aligned}\nx ={}& a+b+c+{} \\\\\\\\\n&d+e+f+g\n\\end{aligned}\n$$\n\n### 公式组\n\n居中\n\n```\n\\begin{gather}\na = b+c+d \\\\\nx = y+z\n\\end{gather}\n```\n\n\n$$\n\\begin{gather}\na = b+c+d \\\\\nx = y+z\n\\end{gather}\n$$\n对齐\n\n```\n\\begin{align}\na &= b+c+d \\\\\nx &= y+z\n\\end{align}\n```\n\n\n$$\n\\begin{align}\na &= b+c+d \\\\\nx &= y+z\n\\end{align}\n$$\n\n### 分段函数\n\n```\ny=\\begin{cases}\n-x,\\quad x\\leq 0 \\\\\\\\\nx,\\quad x>0\n\\end{cases}\n```\n\n\n$$\ny=\\begin{cases}\n-x,\\quad x\\leq 0 \\\\\\\\\nx,\\quad x>0\n\\end{cases}\n$$\n\n### 划线、制表\n\n```\n\\left(\\begin{array}{|c|c|}\n1 & 2 \\\\\n\\hline\n3 & 4\n\\end{array}\\right) \\\\\n\n\\begin{array}{|c|c|}\n\\hline11 & \\cdots \\\\\n\\hline3 & 4 \\\\\n\\hline\n\\end{array}\n```\n\n\n$$\n\\left(\\begin{array}{|c|c|}\n1 & 2 \\\\\n\\hline\n3 & 4\n\\end{array}\\right) \\\\\n\n\\begin{array}{|c|c|}\n\\hline\n11 & \\cdots \\\\\n\\hline\n3 & 4 \\\\\n\\hline\n\\end{array}\n$$\n\n### 希腊字母\n\n```\n\\begin{array}{|c|c|c|c|c|c|c|c|}\n\\hline\n{\\alpha} & {\\backslash alpha} & {\\theta} & {\\backslash theta} & {o} & {o} & {\\upsilon} & {\\backslash upsilon} \\\\\\\\\n\\hline\n{\\beta} & {\\backslash beta} & {\\vartheta} & {\\backslash vartheta} & {\\pi} & {\\backslash pi} & {\\phi} & {\\backslash phi} \\\\\\\\\n\\hline\n{\\gamma} & {\\backslash gamma} & {\\iota} & {\\backslash iota} & {\\varpi} & {\\backslash varpi} & {\\varphi} & {\\backslash varphi} \\\\\\\\\n\\hline\n{\\delta} & {\\backslash delta} & {\\kappa} & {\\backslash kappa} & {\\rho} & {\\backslash rho} & {\\chi} & {\\backslash chi} \\\\\\\\\n\\hline\n{\\epsilon} & {\\backslash epsilon} & {\\lambda} & {\\backslash lambda} & {\\varrho} & {\\backslash varrho} & {\\psi} & {\\backslash psi} \\\\\\\\\n\\hline\n{\\varepsilon} & {\\backslash varepsilon} & {\\mu} & {\\backslash mu} & {\\sigma} & {\\backslash sigma} & {\\omega} & {\\backslash omega} \\\\\\\\\n\\hline\n{\\zeta} & {\\backslash zeta} & {\\nu} & {\\backslash nu} & {\\varsigma} & {\\backslash varsigma} & {} & {} \\\\\\\\\n\\hline\n{\\eta} & {\\backslash eta} & {\\xi} & {\\backslash xi} & {\\tau} & {\\backslash tau} & {} & {} \\\\\\\\\n\\hline\n{\\Gamma} & {\\backslash Gamma} & {\\Lambda} & {\\backslash Lambda} & {\\Sigma} & {\\backslash Sigma} & {\\Psi} & {\\backslash Psi} \\\\\\\\\n\\hline\n{\\Delta} & {\\backslash Delta} & {\\Xi} & {\\backslash Xi} & {\\Upsilon} & {\\backslash Upsilon} & {\\Omega} & {\\backslash Omega} \\\\\\\\\n\\hline\n{\\Omega} & {\\backslash Omega} & {\\Pi} & {\\backslash Pi} & {\\Phi} & {\\backslash Phi} & {} & {} \\\\\\\\\n\\hline\n\\end{array}\n```\n\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|}\n\\hline\n{\\alpha} & {\\backslash alpha} & {\\theta} & {\\backslash theta} & {o} & {o} & {\\upsilon} & {\\backslash upsilon} \\\\\\\\\n\\hline\n{\\beta} & {\\backslash beta} & {\\vartheta} & {\\backslash vartheta} & {\\pi} & {\\backslash pi} & {\\phi} & {\\backslash phi} \\\\\\\\\n\\hline\n{\\gamma} & {\\backslash gamma} & {\\iota} & {\\backslash iota} & {\\varpi} & {\\backslash varpi} & {\\varphi} & {\\backslash varphi} \\\\\\\\\n\\hline\n{\\delta} & {\\backslash delta} & {\\kappa} & {\\backslash kappa} & {\\rho} & {\\backslash rho} & {\\chi} & {\\backslash chi} \\\\\\\\\n\\hline\n{\\epsilon} & {\\backslash epsilon} & {\\lambda} & {\\backslash lambda} & {\\varrho} & {\\backslash varrho} & {\\psi} & {\\backslash psi} \\\\\\\\\n\\hline\n{\\varepsilon} & {\\backslash varepsilon} & {\\mu} & {\\backslash mu} & {\\sigma} & {\\backslash sigma} & {\\omega} & {\\backslash omega} \\\\\\\\\n\\hline\n{\\zeta} & {\\backslash zeta} & {\\nu} & {\\backslash nu} & {\\varsigma} & {\\backslash varsigma} & {} & {} \\\\\\\\\n\\hline\n{\\eta} & {\\backslash eta} & {\\xi} & {\\backslash xi} & {\\tau} & {\\backslash tau} & {} & {} \\\\\\\\\n\\hline\n{\\Gamma} & {\\backslash Gamma} & {\\Lambda} & {\\backslash Lambda} & {\\Sigma} & {\\backslash Sigma} & {\\Psi} & {\\backslash Psi} \\\\\\\\\n\\hline\n{\\Delta} & {\\backslash Delta} & {\\Xi} & {\\backslash Xi} & {\\Upsilon} & {\\backslash Upsilon} & {\\Omega} & {\\backslash Omega} \\\\\\\\\n\\hline\n{\\Omega} & {\\backslash Omega} & {\\Pi} & {\\backslash Pi} & {\\Phi} & {\\backslash Phi} & {} & {} \\\\\\\\\n\\hline\n\\end{array}\n$$\n\n","tags":["学习笔记"]},{"title":"Hexo","url":"/Hexo/","content":"\n## 主文件夹下\n\n- **_config.yml**\n\n  网站的配置信息，您可以在此配置大部分的参数。\n\n- **package.json**\n\n  应用程序的信息。EJS, Stylus 和 Markdown renderer 已默认安装，您可以自由移除。\n\n- **scaffolds**\n\n  模版。当您新建文章时，Hexo 会根据 scaffold 来建立文件。\n\n  Hexo的模板是指在新建的markdown文件中默认填充的内容。例如，如果您修改scaffold/post.md中的Front-matter内容，那么每次新建一篇文章时都会包含这个修改。\n\n- **source**\n\n  资源文件夹是存放用户资源的地方。除 _posts 文件夹之外，开头命名为 _ (下划线)的文件 / 文件夹和隐藏的文件将会被忽略。Markdown 和 HTML 文件会被解析并放到 public 文件夹，而其他文件会被拷贝过去。\n\n- **themes**\n\n  主题文件夹。Hexo 会根据主题来生成静态页面。\n\n## 新建一个文档\n\n```\nhexo new [layout]<title>` # 如果标题包含空格的话，请使用引号括起来\nhexo new \"post title with whitespace\"\n\nhexo generate # 生成静态文件\n\nhexo -d # deploy文件生成后立即部署网站\nhexo -w # watch监视文件变动\n\nhexo server # 启动服务器，在本地查看\n\nhexo clean # 清除缓存文件 (db.json) 和已生成的静态文件 (public）\n\n\n```\n\n## 其他代码测试\n\n- [x] 信息论作业\n\n- [ ] 数据挖掘知识整理\n\n- [x] VPN搭建\n\n- [ ] 书法知识\n\n- [ ] python爬虫\n\n> 测试aagoieng奥诶你饿哦更IEN个我干呢哦我表格五更诶包是借款的掠夺发呢了诶费更而我就够额诶结构诶个\n>\n> 绿色康\n\n1. 安利你偶尔\n\n2. 斯丁哦[^1]\n\n3. 电视`里能`[^2]\n\n4. 僧\n\n5. 孙工二**十个**\n\n   `if else when{}`<or>if else when</or>\n\n   ```java\n   public static void main(){\n   \n   }\n   ```\n\n\n- [设法贵宾卡](http://www.jiziti.fun)\n- <u>吴邦国</u>\n- <or>为改</or>*变你es*\n- 苦涩个\n- ~~深刻的风格就饿~~\n\n[^1]: 第一次使用脚注\n[^2]: 其他\n\n","tags":["学习笔记"]}]